{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0625d030",
   "metadata": {},
   "source": [
    "# Analyse de LMGEC\n",
    "\n",
    "> Article analysÃ© : **LMGEC: Simultaneous Linear Multi-view Attributed Graph Representation Learning and Clustering**, WSDM 2023.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## ProblÃ©matique centrale  :  \n",
    "LMGEC rÃ©pond Ã  la problÃ©matique suivante :\n",
    "â€œComment apprendre des reprÃ©sentations partagÃ©es et effectuer un clustering efficace sur des graphes multi-vues attribuÃ©s, tout en garantissant simplicitÃ©, rapiditÃ©, et robustesse face Ã  lâ€™hÃ©tÃ©rogÃ©nÃ©itÃ© des vues ?â€\n",
    ">\n",
    "\n",
    "![Alt text](problem.png)\n",
    "\n",
    "\n",
    "## ğŸ“… 1. MÃ©thodologie : quel type de fusion ?\n",
    "\n",
    "LMGEC repose sur une **fusion linÃ©aire pondÃ©e tardive** des vues. \n",
    "\n",
    "- Chaque vue est d'abord **filtrÃ©e localement** (1-hop) pour lisser les attributs : $$ H_v = S_v X_v $$\n",
    "- Une **pondÃ©ration adaptative** des vues est appliquÃ©e via des poids $$ \\alpha_v $$, calculÃ©s dynamiquement.\n",
    "- Les embeddings obtenus sont projetÃ©s et **fusionnÃ©s dans un espace commun** pour effectuer le clustering.\n",
    "\n",
    "> **Type de fusion :** tardive + pondÃ©ration adaptative (soft fusion)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” 2. HypothÃ¨ses sur les vues\n",
    "\n",
    "- âœ… MÃªmes nÅ“uds dans toutes les vues\n",
    "- âœ… Vues potentiellement **trÃ¨s hÃ©tÃ©rogÃ¨nes** (topologies ou attributs)\n",
    "- âŒ Pas de traitement spÃ©cial des vues **manquantes** ou dÃ©salignÃ©es\n",
    "- âœ… Le modÃ¨le peut attÃ©nuer les vues bruitÃ©es via la pondÃ©ration $$ \\alpha_v $$\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š 3. ModÃ¨les mathÃ©matiques\n",
    "\n",
    "- **Filtrage de chaque vue :** $$ H_v = S_v X_v $$, oÃ¹ $$ S_v = \\tilde{D}^{-1} (\\tilde{A}_v) $$ avec self-loops.\n",
    "\n",
    "- **Objectif :**\n",
    "$$\n",
    "\\min_{G, F, W_1,\\dots,W_V} \\sum_{v=1}^{V} \\alpha_v \\| H_v - G F W_v^\\top \\|^2\n",
    "$$\n",
    "Avec :\n",
    "  - $ G \\in \\{0,1\\}^{n \\times k} $ : clustering (soft or hard)\n",
    "  - $ W_v \\in \\mathbb{R}^{d \\times f} $, $ W_v W_v^\\top = I $\n",
    "\n",
    "- **PondÃ©ration des vues :**\n",
    "$$\n",
    "\\alpha_v = \\text{softmax}\\left(-\\frac{I_v}{\\tau}\\right), \\quad I_v = \\| H_v - G_v F_v \\|\n",
    "$$\n",
    "\n",
    "> Optimisation par **Bloc Coordinate Descent**\n",
    "\n",
    "> \n",
    "![Alt text](recon.png)\n",
    "---\n",
    "\n",
    "## ğŸ“ˆ 4. Types de donnÃ©es utilisÃ©es\n",
    "\n",
    "- **Topologies diffÃ©rentes, mÃªmes features :** ACM, DBLP, IMDB\n",
    "- **MÃªmes topologies, features diffÃ©rentes :** Amazon Photos\n",
    "- **Topologies + features diffÃ©rentes :** Wiki\n",
    "\n",
    "> LMGEC couvre **tous les cas multi-vues usuels**\n",
    "> \n",
    "![Alt text](topo.png)\n",
    "---\n",
    "\n",
    "## ğŸ“Š 5. MÃ©triques dâ€™Ã©valuation\n",
    "\n",
    "Les performances sont mesurÃ©es avec 4 mÃ©triques standards en clustering :\n",
    "\n",
    "| MÃ©trique | Description |\n",
    "|----------|-------------|\n",
    "| **NMI** | Normalized Mutual Information |\n",
    "| **ARI** | Adjusted Rand Index |\n",
    "| **ACC** | Accuracy (appliquÃ©e au clustering) |\n",
    "| **F1-score** | PrÃ©cision + rappel |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# ğŸ“Š MÃ©triques dâ€™Ã©valuation pour le clustering non supervisÃ©\n",
    "\n",
    "Lorsque jâ€™Ã©value un algorithme de clustering (comme LMGEC), je dois utiliser des mÃ©triques quantitatives pour mesurer Ã  quel point les clusters trouvÃ©s sont cohÃ©rents avec les classes rÃ©elles (si elles sont disponibles). Voici les principales mÃ©triques utilisÃ©es :\n",
    "\n",
    "---\n",
    "\n",
    "| **MÃ©trique** | **Nom complet** | **Ce quâ€™elle mesure** | **Valeurs typiques** |\n",
    "|--------------|------------------|------------------------|-----------------------|\n",
    "| **NMI** | Normalized Mutual Information | Le degrÃ© de similaritÃ© entre les clusters prÃ©dits et les vraies classes, basÃ© sur lâ€™information partagÃ©e | Entre 0 (aucune info) et 1 (parfait) |\n",
    "| **ARI** | Adjusted Rand Index | Ã€ quel point deux partitions (clusters vs classes) sont en accord, corrigÃ© pour le hasard | Entre -1 (pire que hasard) et 1 (parfait accord) |\n",
    "| **ACC** | Clustering Accuracy | Pourcentage dâ€™exemples correctement classÃ©s, aprÃ¨s rÃ©assignation optimale des labels | Entre 0 et 1 (ou 0% Ã  100%) |\n",
    "| **F1-score** | F1 Measure (harmonique prÃ©cision / rappel) | Moyenne harmonique entre la prÃ©cision (exactitude des clusters) et le rappel (couverture des vraies classes) | Entre 0 et 1 |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  DÃ©tails complÃ©mentaires\n",
    "\n",
    "### ğŸ”¹ NMI (Normalized Mutual Information)\n",
    "- Formule :\n",
    "$$\n",
    "\\text{NMI}(Y, C) = \\frac{2 \\, I(Y; C)}{H(Y) + H(C)}\n",
    "$$\n",
    "avec \\( I(Y; C) \\) l'information mutuelle entre la partition vraie \\( Y \\) et prÃ©dite \\( C \\).\n",
    "- Avantage : **invariante aux permutations de labels**.\n",
    "\n",
    "### ğŸ”¹ ARI (Adjusted Rand Index)\n",
    "- Tient compte du **nombre de paires** correctement regroupÃ©es ou sÃ©parÃ©es.\n",
    "- Corrige le **Rand Index** pour compenser le regroupement alÃ©atoire.\n",
    "- Avantage : bonne robustesse mÃªme avec des dÃ©sÃ©quilibres de classe.\n",
    "\n",
    "### ğŸ”¹ ACC (Accuracy)\n",
    "- Mesure directe et intuitive.\n",
    "- Requiert une **permutation optimale des clusters** (car les labels peuvent Ãªtre inversÃ©s).\n",
    "\n",
    "### ğŸ”¹ F1-score\n",
    "- Combine deux notions :\n",
    "  - **PrÃ©cision** : parmi les Ã©lÃ©ments du cluster, combien sont bien classÃ©s ?\n",
    "  - **Rappel** : parmi les vrais Ã©lÃ©ments dâ€™une classe, combien sont capturÃ©s ?\n",
    "- Lâ€™Ã©quilibre entre les deux est utile surtout pour les classes dÃ©sÃ©quilibrÃ©es.\n",
    "\n",
    "---\n",
    "\n",
    "> En rÃ©sumÃ© : utiliser plusieurs mÃ©triques permet dâ€™avoir une vue plus complÃ¨te de la qualitÃ© du clustering. NMI et ARI sont les plus stables pour comparer les mÃ©thodes, tandis que ACC et F1 donnent une interprÃ©tation plus intuitive des performances.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ğŸ¯ Contribution originale et Ã©valuation critique de LMGEC\n",
    "\n",
    "## ğŸ§© Quelle est la **contribution originale** du papier ?\n",
    "\n",
    "Lâ€™article propose **LMGEC**, un modÃ¨le linÃ©aire simple et efficace pour :\n",
    "\n",
    "- RÃ©aliser **simultanÃ©ment** lâ€™apprentissage de reprÃ©sentation et le clustering sur des graphes multi-vues attribuÃ©s.\n",
    "- Offrir une **formulation unifiÃ©e** intÃ©grant :\n",
    "  - une Ã©tape de **propagation locale (1-hop)** par un filtre de graphe linÃ©aire,\n",
    "  - un **mÃ©canisme de pondÃ©ration adaptative des vues** (softmax sur lâ€™inertie),\n",
    "  - un objectif combinÃ© de **reconstruction + clustering**.\n",
    "- ÃŠtre **gÃ©nÃ©rique**, applicable Ã  :\n",
    "  - plusieurs graphes avec une mÃªme matrice de features,\n",
    "  - plusieurs matrices de features sur un seul graphe,\n",
    "  - ou un mÃ©lange des deux (cas du dataset Wiki).\n",
    "- ÃŠtre **beaucoup plus rapide** que les modÃ¨les existants tout en offrant des performances comparables, voire meilleures.\n",
    "- Fournir une **analyse mathÃ©matique et expÃ©rimentale approfondie** ainsi que le **code open-source**.\n",
    "\n",
    "## âœ… **Points forts** de LMGEC\n",
    "\n",
    "| Atout | DÃ©tail |\n",
    "|-------|--------|\n",
    "| âœ… SimplicitÃ© | Formulation linÃ©aire claire et interprÃ©table |\n",
    "| âœ… EfficacitÃ© | Temps d'entraÃ®nement **jusqu'Ã  10Ã— plus rapide** que les mÃ©thodes GCN ou autoencoder |\n",
    "| âœ… Robustesse | CapacitÃ© Ã  ignorer les vues peu informatives via le mÃ©canisme d'attention/inertie |\n",
    "| âœ… GÃ©nÃ©ralitÃ© | Supporte diffÃ©rents types de graphes multi-vues sans contraintes |\n",
    "| âœ… Formulation unifiÃ©e | Apprentissage de reprÃ©sentation + clustering dans un mÃªme objectif |\n",
    "| âœ… ReproductibilitÃ© | Code disponible en open-source et rÃ©sultats dÃ©taillÃ©s sur 5 benchmarks |\n",
    "\n",
    "## âš ï¸ **Limites** de LMGEC\n",
    "\n",
    "| Limite | DÃ©tail |\n",
    "|--------|--------|\n",
    "| âŒ MÃ©thode linÃ©aire | Ne capture pas les non-linÃ©aritÃ©s complexes, contrairement aux modÃ¨les deep |\n",
    "| âŒ Pas de gestion des vues manquantes | Chaque vue est supposÃ©e complÃ¨te et bien alignÃ©e |\n",
    "| âŒ Risque de sur-lissage Ã©vitÃ© uniquement via un filtrage 1-hop | Ce choix reste rigide dans certains cas |\n",
    "| âŒ Pas de mÃ©canisme d'apprentissage end-to-end avec supervision Ã©ventuelle | ModÃ¨le strictement non supervisÃ© |\n",
    "| âŒ Pas de mÃ©canisme explicite de fusion dynamique | Le poids est fixÃ© aprÃ¨s initialisation (pas appris pendant optimisation) |\n",
    "\n",
    "## ğŸ§  Conclusion\n",
    "LMGEC se positionne comme une **alternative simple, rapide et robuste** aux mÃ©thodes complexes basÃ©es sur GCN ou autoencodeurs. Il est particuliÃ¨rement pertinent dans des contextes contraints en ressources ou nÃ©cessitant une interprÃ©tabilitÃ© forte.\n",
    "\n",
    "Cependant, pour des cas trÃ¨s non-linÃ©aires ou avec des donnÃ©es partiellement alignÃ©es, des mÃ©thodes plus expressives comme les modÃ¨les Ã  attention ou graph contrastif peuvent Ãªtre prÃ©fÃ©rÃ©es.\n",
    "\n",
    "## ğŸŒ RÃ©capitulatif SynthÃ©tique\n",
    "\n",
    "| Ã‰lÃ©ment | DÃ©scription |\n",
    "|--------|-------------|\n",
    "| **Fusion** | Tardive, linÃ©aire, pondÃ©ration adaptative |\n",
    "| **HypothÃ¨ses sur les vues** | MÃªmes nÅ“uds, hÃ©tÃ©rogÃ©nÃ©itÃ© supportÃ©e |\n",
    "| **Formulation** | LinÃ©aire, objectif joint reconstruction + clustering |\n",
    "| **DonnÃ©es** | Multi-vues topologiques, attributaires ou mixtes |\n",
    "| **MÃ©triques** | NMI, ARI, ACC, F1 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ce8ac",
   "metadata": {},
   "source": [
    "# Analyse de lâ€™article \"Efficient Graph Convolution for Joint Node Representation Learning and Clustering\" (Fettal et al., WSDM 2022)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. ProblÃ©matique centrale\n",
    "Lâ€™article traite le problÃ¨me de lâ€™**apprentissage conjoint des reprÃ©sentations de nÅ“uds et du clustering dans les graphes attribuÃ©s**. Lâ€™objectif est dâ€™amÃ©liorer la qualitÃ© du clustering en intÃ©grant directement lâ€™objectif de regroupement dans la phase dâ€™apprentissage des embeddings, tout en garantissant une **efficacitÃ© computationnelle**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Type de fusion\n",
    "- **Type :** IntermÃ©diaire  \n",
    "  La fusion des vues (structure + attributs) se fait via une fonction dâ€™agrÃ©gation `agg(A, X) = Táµ–X`, câ€™est-Ã -dire une **fusion intermÃ©diaire**.\n",
    "  \n",
    "- **Nature :** LinÃ©aire  \n",
    "  La fusion est **linÃ©aire** : l'agrÃ©gation est une multiplication de matrices entre une propagation modifiÃ©e $$T^p$$ et la matrice d'attributs $$X$$.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. PondÃ©ration des vues\n",
    "- Il nâ€™y a **pas de pondÃ©ration explicite des vues**. Le modÃ¨le utilise une **fusion fixe** de la structure (via le filtre $$T^p$$) et des attributs sans mÃ©canisme dâ€™attention ou de softmax.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. HypothÃ¨ses sur les vues\n",
    "- Les vues **partagent les mÃªmes nÅ“uds** (les attributs sont associÃ©s aux mÃªmes sommets que ceux du graphe).\n",
    "- Les vues peuvent avoir des **topologies diffÃ©rentes** (on peut appliquer diffÃ©rentes puissances de propagation $$p$$).\n",
    "- Le modÃ¨le **ne traite pas explicitement les vues manquantes ou dÃ©salignÃ©es**.\n",
    "- Il **rÃ©duit lâ€™impact des vues bruitÃ©es** indirectement via le filtrage **low-pass** du signal (par normalisation spectrale du filtre).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Formalisme mathÃ©matique du modÃ¨le\n",
    "\n",
    "### Fonction objectif (Eq. (1)) :\n",
    "$$\n",
    "\\min_{\\theta_1,\\theta_2,G,F} \\|\\text{dec}_{\\theta_2}(\\text{enc}_{\\theta_1}(T^p X)) - T^p X\\|^2 + \\alpha \\|\\text{enc}_{\\theta_1}(T^p X) - GF\\|^2\n",
    "$$\n",
    "\n",
    "- **1er terme :** reconstruction (type autoencodeur linÃ©aire)\n",
    "- **2e terme :** rÃ©gularisation de clustering (k-means sur les embeddings)\n",
    "\n",
    "### Contrainte :\n",
    "$$\n",
    "G \\in \\{0,1\\}^{n \\times k}, \\quad G1_k = 1_n\n",
    "$$\n",
    "(matrice dâ€™affectation de clusters : chaque nÅ“ud appartient Ã  un et un seul cluster)\n",
    "\n",
    "### Reformulation (Eq. (5), (6)) :\n",
    "HypothÃ¨se que $$W = W_1 = W_2^T$$, et que $$W^T W = I_k$$ (orthogonalitÃ©), on obtient :\n",
    "\n",
    "$$\n",
    "\\min_{G,F,W} \\|T^p X - GFW^T\\|^2, \\quad \\text{s.c. } G \\in \\{0,1\\}^{n \\times k}, \\, G1_k = 1_n, \\, W^T W = I_k\n",
    "$$\n",
    "\n",
    "### Matrices utilisÃ©es :\n",
    "- $ T $ : Matrice de propagation normalisÃ©e (modification de SGC)\n",
    "- $ T^p X $ : signal propagÃ©\n",
    "- $ G $ : matrice dâ€™assignation\n",
    "- $ F $ : centroÃ¯des\n",
    "- $ W $ : projection orthogonale\n",
    "\n",
    "### Optimisation :\n",
    "- Alternance : mise Ã  jour de $$G$$, $$F$$, et $$W$$ par descente coordonnÃ©e\n",
    "- MÃ©thodes :\n",
    "  - $ F $ : least squares\n",
    "  - $ W $ : Procrustes (SVD)\n",
    "  - $ G $ : k-means\n",
    "\n",
    "---\n",
    "\n",
    "## 6. DonnÃ©es utilisÃ©es\n",
    "- DonnÃ©es **rÃ©elles** : Cora, Citeseer, Pubmed, Wiki\n",
    "- Les graphes ont les **mÃªmes nÅ“uds** dans structure et attributs\n",
    "- **Topologies diffÃ©rentes** : oui, entre structure (A) et attributs (X)\n",
    "- Pas de **vues manquantes**, toutes les matrices sont complÃ¨tes\n",
    "\n",
    "---\n",
    "\n",
    "## 7. ExpÃ©riences\n",
    "- **MÃ©triques** : Accuracy, NMI, F1-score\n",
    "- **Ã‰tudes dâ€™ablation** :\n",
    "  - Impact de la **matrice de propagation** (comparaison avec normalisations classiques)\n",
    "  - Influence du **paramÃ¨tre de propagation** $$p$$\n",
    "- **Robustesse au bruit** : pas explicitement testÃ©e\n",
    "- **Transductif** : le modÃ¨le apprend sur le graphe complet (pas inductif)\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Avantages mis en avant\n",
    "- **Formulation simple** et **analytique**\n",
    "- **ComplexitÃ© faible** $$O(p|E|d + tndk)$$\n",
    "- **Performances comparables ou meilleures** que SOTA\n",
    "- **Visualisation claire** via embeddings structurÃ©s ($$R^2$$ Ã©levÃ©)\n",
    "- **Facilement implÃ©mentable** et rapide sur GPU\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Limitations\n",
    "- **ModÃ¨le transductif**, pas dâ€™extension inductive\n",
    "- **ParamÃ¨tre $$p$$** sensible (propagation) â†’ nÃ©cessite une sÃ©lection heuristique\n",
    "- Pas de gestion explicite des **vues manquantes** ou des **donnÃ©es bruitÃ©es**\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Perspectives dâ€™amÃ©lioration\n",
    "- Ã‰tendre Ã  une **formulation inductive**\n",
    "- Ajouter un **mÃ©canisme adaptatif pour le poids $$\\alpha$$** entre reconstruction et clustering\n",
    "- Envisager des **versions co-clustering** pour documents/entitÃ©s bipartites\n",
    "- Ajouter **pondÃ©ration ou attention** entre structure et attributs\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef69d09",
   "metadata": {},
   "source": [
    "# Analyse de lâ€™article : Efficient Graph Convolution for Joint Node Representation Learning and Clustering (GCC)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Quelle est la problÃ©matique centrale Ã  laquelle rÃ©pond lâ€™article ?\n",
    "\n",
    "Lâ€™article traite **du clustering de graphes attribuÃ©s**, oÃ¹ chaque nÅ“ud a des attributs. Le dÃ©fi est de faire **lâ€™apprentissage de reprÃ©sentations et le clustering conjointement**, plutÃ´t que de maniÃ¨re sÃ©quentielle (tandem). La mÃ©thode vise Ã  produire des **reprÃ©sentations adaptÃ©es Ã  la structure de clusters**, tout en Ã©tant **efficace computationnellement**.\n",
    "\n",
    "- ğŸ§  **DÃ©fi principal :** clustering efficace et conjoint avec lâ€™apprentissage de reprÃ©sentations dans les graphes attribuÃ©s, avec une complexitÃ© rÃ©duite.\n",
    "\n",
    "![Q1](2-1.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Quel type de fusion est utilisÃ© pour combiner les diffÃ©rentes vues ?\n",
    "\n",
    "MÃªme si GCC ne traite pas explicitement de donnÃ©es multivues, il applique une fusion **prÃ©coce linÃ©aire** entre la topologie $A$ et les attributs $X$, via une opÃ©ration dâ€™agrÃ©gation :\n",
    "\n",
    "- Type de fusion : **prÃ©coce**\n",
    "- Nature de la fusion : **linÃ©aire**\n",
    "- AgrÃ©gation : $$ \\text{agg}(A, X) = T^p X $$\n",
    "\n",
    "![Q2](2-2.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Y a-t-il une pondÃ©ration des vues ?\n",
    "\n",
    "GCC ne traite quâ€™un seul graphe, donc **pas de pondÃ©ration explicite entre vues**. Toutefois, le paramÃ¨tre $p$ contrÃ´le combien de \"hops\" sont pris en compte, ce qui agit comme une **pondÃ©ration spatiale implicite**.\n",
    "\n",
    "- âŒ Pas de softmax, attention ou pondÃ©ration dynamique.\n",
    "- âœ… Influence implicite via $p$.\n",
    "\n",
    "![Q3](2-3.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Quelles sont les hypothÃ¨ses faites sur les vues ?\n",
    "\n",
    "MÃªme dans un cadre monovue, GCC repose sur des hypothÃ¨ses implicites :\n",
    "\n",
    "- âœ… Les vues (topologie + attributs) **partagent les mÃªmes nÅ“uds**.\n",
    "- âŒ Ne supporte pas de **vues hÃ©tÃ©rogÃ¨nes** ou **dÃ©salignÃ©es**.\n",
    "- âŒ Ne gÃ¨re pas les **vues manquantes**.\n",
    "- âŒ Pas de mÃ©canisme explicite contre le **bruit inter-vue**.\n",
    "\n",
    "![Q4](2-4.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Quel est le formalisme mathÃ©matique du modÃ¨le ?\n",
    "\n",
    "Le modÃ¨le repose sur une **optimisation conjointe** dâ€™une perte de reconstruction et dâ€™une rÃ©gularisation de clustering :\n",
    "\n",
    "### Fonction objectif principale :\n",
    "$$\n",
    "\\min_{G, F, W} \\| T^p X - T^p X W W^\\top \\|^2 + \\alpha \\| T^p X W - G F \\|^2\n",
    "$$\n",
    "\n",
    "### Avec contraintes :\n",
    "- $W^\\top W = I_k$\n",
    "- $G \\in \\{0, 1\\}^{n \\times k}$ et $G \\mathbf{1}_k = \\mathbf{1}_n$\n",
    "\n",
    "On peut reformuler en :\n",
    "$$\n",
    "\\min_{G, F, W} \\| T^p X - G F W^\\top \\|^2\n",
    "$$\n",
    "\n",
    "### MÃ©thode dâ€™optimisation :\n",
    "- Alternance :\n",
    "  - $F \\leftarrow (G^\\top G)^{-1} G^\\top T^p X W$\n",
    "  - $W \\leftarrow \\text{SVD}( (T^p X)^\\top G F )$\n",
    "  - $G \\leftarrow \\arg\\min_j \\| (T^p X W)_i - f_j \\|^2$\n",
    "\n",
    "![Q5](2-5.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Quel type de donnÃ©es est utilisÃ© pour lâ€™Ã©valuation ?\n",
    "\n",
    "Les jeux de donnÃ©es sont **rÃ©els**, avec une topologie fixe et des attributs diffÃ©rents selon les graphes :\n",
    "\n",
    "| Dataset   | # Nodes | # Edges | # Features | # Classes |\n",
    "|-----------|---------|---------|------------|-----------|\n",
    "| Citeseer  | 3327    | 4732    | 3703       | 6         |\n",
    "| Cora      | 2708    | 5429    | 1433       | 7         |\n",
    "| Pubmed    | 19717   | 44338   | 500        | 3         |\n",
    "| Wiki      | 2405    | 17981   | 4973       | 17        |\n",
    "\n",
    "- âœ… MÃªme nÅ“uds, topologies fixes\n",
    "- âœ… Attributs textuels\n",
    "- âœ… Jeux de donnÃ©es entiÃ¨rement rÃ©els\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Quelles expÃ©riences sont menÃ©es pour valider le modÃ¨le ?\n",
    "\n",
    "- **Comparaison avec les baselines** (S2GC, GIC, DCN, etc.) sur ACC, NMI, F1\n",
    "- **Ablation sur la normalisation** utilisÃ©e pour $T$\n",
    "- **Visualisation des embeddings** via t-SNE + analyse RÂ²\n",
    "- **Temps dâ€™exÃ©cution** comparÃ© aux autres mÃ©thodes (AGE, S2GCâ€¦)\n",
    "- **SÃ©lection automatique du paramÃ¨tre** $p$\n",
    "\n",
    "> Le modÃ¨le est **transductif**, pas inductif.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Quels sont les avantages mis en avant par les auteurs ?\n",
    "\n",
    "- âœ… **SimplicitÃ©** (modÃ¨le linÃ©aire, formulation analytique)\n",
    "- âœ… **RapiditÃ©** (SVD, closed-form updates)\n",
    "- âœ… **Optimisation dÃ©terministe** (descente garantie)\n",
    "- âœ… **ReprÃ©sentations de qualitÃ©** pour clustering et visualisation\n",
    "- âœ… **ScalabilitÃ©** prouvÃ©e sur des graphes de grande taille (ex : Pubmed)\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Quelles sont les limitations ou points faibles du modÃ¨le ?\n",
    "\n",
    "- âŒ Pas de version **inductive**\n",
    "- âŒ Sensible Ã  lâ€™**initialisation**\n",
    "- âŒ $p$ doit Ãªtre **choisi heuristiquement**\n",
    "- âŒ Ne supporte pas **vues multiples**, bruitÃ©es, ou **manquantes**\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Quelles sont les perspectives dâ€™amÃ©lioration discutÃ©es ?\n",
    "\n",
    "- ğŸ”„ Optimisation du **coefficient $\\alpha$** entre reconstruction et clustering\n",
    "- ğŸ”® DÃ©veloppement dâ€™une version **inductive**\n",
    "- ğŸ§© Extension vers le **co-clustering** (noeuds + attributs)\n",
    "- ğŸŒ Adaptation plus fine du **paramÃ¨tre $p$**\n",
    "\n",
    "---\n",
    "\n",
    "**Fin du rapport.**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
