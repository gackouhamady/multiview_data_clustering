{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0625d030",
   "metadata": {},
   "source": [
    "# Analyse de LMGEC\n",
    "\n",
    "> Article analys√© : **LMGEC: Simultaneous Linear Multi-view Attributed Graph Representation Learning and Clustering**, WSDM 2023.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Probl√©matique centrale  :  \n",
    "LMGEC r√©pond √† la probl√©matique suivante :\n",
    "‚ÄúComment apprendre des repr√©sentations partag√©es et effectuer un clustering efficace sur des graphes multi-vues attribu√©s, tout en garantissant simplicit√©, rapidit√©, et robustesse face √† l‚Äôh√©t√©rog√©n√©it√© des vues ?‚Äù\n",
    ">\n",
    "\n",
    "![Alt text](problem.png)\n",
    "\n",
    "\n",
    "## üìÖ 1. M√©thodologie : quel type de fusion ?\n",
    "\n",
    "LMGEC repose sur une **fusion lin√©aire pond√©e tardive** des vues. \n",
    "\n",
    "- Chaque vue est d'abord **filtr√©e localement** (1-hop) pour lisser les attributs : $$ H_v = S_v X_v $$\n",
    "- Une **pond√©ration adaptative** des vues est appliqu√©e via des poids $$ \\alpha_v $$, calcul√©s dynamiquement.\n",
    "- Les embeddings obtenus sont projet√©s et **fusionn√©s dans un espace commun** pour effectuer le clustering.\n",
    "\n",
    "> **Type de fusion :** tardive + pond√©ration adaptative (soft fusion)\n",
    "\n",
    "---\n",
    "\n",
    "## üîç 2. Hypoth√®ses sur les vues\n",
    "\n",
    "- ‚úÖ M√™mes n≈ìuds dans toutes les vues\n",
    "- ‚úÖ Vues potentiellement **tr√®s h√©t√©rog√®nes** (topologies ou attributs)\n",
    "- ‚ùå Pas de traitement sp√©cial des vues **manquantes** ou d√©salign√©es\n",
    "- ‚úÖ Le mod√®le peut att√©nuer les vues bruit√©es via la pond√©ration $$ \\alpha_v $$\n",
    "\n",
    "---\n",
    "\n",
    "## üìä 3. Mod√®les math√©matiques\n",
    "\n",
    "- **Filtrage de chaque vue :** $$ H_v = S_v X_v $$, o√π $$ S_v = \\tilde{D}^{-1} (\\tilde{A}_v) $$ avec self-loops.\n",
    "\n",
    "- **Objectif :**\n",
    "$$\n",
    "\\min_{G, F, W_1,\\dots,W_V} \\sum_{v=1}^{V} \\alpha_v \\| H_v - G F W_v^\\top \\|^2\n",
    "$$\n",
    "Avec :\n",
    "  - $ G \\in \\{0,1\\}^{n \\times k} $ : clustering (soft or hard)\n",
    "  - $ W_v \\in \\mathbb{R}^{d \\times f} $, $ W_v W_v^\\top = I $\n",
    "\n",
    "- **Pond√©ration des vues :**\n",
    "$$\n",
    "\\alpha_v = \\text{softmax}\\left(-\\frac{I_v}{\\tau}\\right), \\quad I_v = \\| H_v - G_v F_v \\|\n",
    "$$\n",
    "\n",
    "> Optimisation par **Bloc Coordinate Descent**\n",
    "\n",
    "> \n",
    "![Alt text](recon.png)\n",
    "---\n",
    "\n",
    "## üìà 4. Types de donn√©es utilis√©es\n",
    "\n",
    "- **Topologies diff√©rentes, m√™mes features :** ACM, DBLP, IMDB\n",
    "- **M√™mes topologies, features diff√©rentes :** Amazon Photos\n",
    "- **Topologies + features diff√©rentes :** Wiki\n",
    "\n",
    "> LMGEC couvre **tous les cas multi-vues usuels**\n",
    "> \n",
    "![Alt text](topo.png)\n",
    "---\n",
    "\n",
    "## üìä 5. M√©triques d‚Äô√©valuation\n",
    "\n",
    "Les performances sont mesur√©es avec 4 m√©triques standards en clustering :\n",
    "\n",
    "| M√©trique | Description |\n",
    "|----------|-------------|\n",
    "| **NMI** | Normalized Mutual Information |\n",
    "| **ARI** | Adjusted Rand Index |\n",
    "| **ACC** | Accuracy (appliqu√©e au clustering) |\n",
    "| **F1-score** | Pr√©cision + rappel |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# üìä M√©triques d‚Äô√©valuation pour le clustering non supervis√©\n",
    "\n",
    "Lorsque j‚Äô√©value un algorithme de clustering (comme LMGEC), je dois utiliser des m√©triques quantitatives pour mesurer √† quel point les clusters trouv√©s sont coh√©rents avec les classes r√©elles (si elles sont disponibles). Voici les principales m√©triques utilis√©es :\n",
    "\n",
    "---\n",
    "\n",
    "| **M√©trique** | **Nom complet** | **Ce qu‚Äôelle mesure** | **Valeurs typiques** |\n",
    "|--------------|------------------|------------------------|-----------------------|\n",
    "| **NMI** | Normalized Mutual Information | Le degr√© de similarit√© entre les clusters pr√©dits et les vraies classes, bas√© sur l‚Äôinformation partag√©e | Entre 0 (aucune info) et 1 (parfait) |\n",
    "| **ARI** | Adjusted Rand Index | √Ä quel point deux partitions (clusters vs classes) sont en accord, corrig√© pour le hasard | Entre -1 (pire que hasard) et 1 (parfait accord) |\n",
    "| **ACC** | Clustering Accuracy | Pourcentage d‚Äôexemples correctement class√©s, apr√®s r√©assignation optimale des labels | Entre 0 et 1 (ou 0% √† 100%) |\n",
    "| **F1-score** | F1 Measure (harmonique pr√©cision / rappel) | Moyenne harmonique entre la pr√©cision (exactitude des clusters) et le rappel (couverture des vraies classes) | Entre 0 et 1 |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† D√©tails compl√©mentaires\n",
    "\n",
    "### üîπ NMI (Normalized Mutual Information)\n",
    "- Formule :\n",
    "$$\n",
    "\\text{NMI}(Y, C) = \\frac{2 \\, I(Y; C)}{H(Y) + H(C)}\n",
    "$$\n",
    "avec \\( I(Y; C) \\) l'information mutuelle entre la partition vraie \\( Y \\) et pr√©dite \\( C \\).\n",
    "- Avantage : **invariante aux permutations de labels**.\n",
    "\n",
    "### üîπ ARI (Adjusted Rand Index)\n",
    "- Tient compte du **nombre de paires** correctement regroup√©es ou s√©par√©es.\n",
    "- Corrige le **Rand Index** pour compenser le regroupement al√©atoire.\n",
    "- Avantage : bonne robustesse m√™me avec des d√©s√©quilibres de classe.\n",
    "\n",
    "### üîπ ACC (Accuracy)\n",
    "- Mesure directe et intuitive.\n",
    "- Requiert une **permutation optimale des clusters** (car les labels peuvent √™tre invers√©s).\n",
    "\n",
    "### üîπ F1-score\n",
    "- Combine deux notions :\n",
    "  - **Pr√©cision** : parmi les √©l√©ments du cluster, combien sont bien class√©s ?\n",
    "  - **Rappel** : parmi les vrais √©l√©ments d‚Äôune classe, combien sont captur√©s ?\n",
    "- L‚Äô√©quilibre entre les deux est utile surtout pour les classes d√©s√©quilibr√©es.\n",
    "\n",
    "---\n",
    "\n",
    "> En r√©sum√© : utiliser plusieurs m√©triques permet d‚Äôavoir une vue plus compl√®te de la qualit√© du clustering. NMI et ARI sont les plus stables pour comparer les m√©thodes, tandis que ACC et F1 donnent une interpr√©tation plus intuitive des performances.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# üéØ Contribution originale et √©valuation critique de LMGEC\n",
    "\n",
    "## üß© Quelle est la **contribution originale** du papier ?\n",
    "\n",
    "L‚Äôarticle propose **LMGEC**, un mod√®le lin√©aire simple et efficace pour :\n",
    "\n",
    "- R√©aliser **simultan√©ment** l‚Äôapprentissage de repr√©sentation et le clustering sur des graphes multi-vues attribu√©s.\n",
    "- Offrir une **formulation unifi√©e** int√©grant :\n",
    "  - une √©tape de **propagation locale (1-hop)** par un filtre de graphe lin√©aire,\n",
    "  - un **m√©canisme de pond√©ration adaptative des vues** (softmax sur l‚Äôinertie),\n",
    "  - un objectif combin√© de **reconstruction + clustering**.\n",
    "- √ätre **g√©n√©rique**, applicable √† :\n",
    "  - plusieurs graphes avec une m√™me matrice de features,\n",
    "  - plusieurs matrices de features sur un seul graphe,\n",
    "  - ou un m√©lange des deux (cas du dataset Wiki).\n",
    "- √ätre **beaucoup plus rapide** que les mod√®les existants tout en offrant des performances comparables, voire meilleures.\n",
    "- Fournir une **analyse math√©matique et exp√©rimentale approfondie** ainsi que le **code open-source**.\n",
    "\n",
    "## ‚úÖ **Points forts** de LMGEC\n",
    "\n",
    "| Atout | D√©tail |\n",
    "|-------|--------|\n",
    "| ‚úÖ Simplicit√© | Formulation lin√©aire claire et interpr√©table |\n",
    "| ‚úÖ Efficacit√© | Temps d'entra√Ænement **jusqu'√† 10√ó plus rapide** que les m√©thodes GCN ou autoencoder |\n",
    "| ‚úÖ Robustesse | Capacit√© √† ignorer les vues peu informatives via le m√©canisme d'attention/inertie |\n",
    "| ‚úÖ G√©n√©ralit√© | Supporte diff√©rents types de graphes multi-vues sans contraintes |\n",
    "| ‚úÖ Formulation unifi√©e | Apprentissage de repr√©sentation + clustering dans un m√™me objectif |\n",
    "| ‚úÖ Reproductibilit√© | Code disponible en open-source et r√©sultats d√©taill√©s sur 5 benchmarks |\n",
    "\n",
    "## ‚ö†Ô∏è **Limites** de LMGEC\n",
    "\n",
    "| Limite | D√©tail |\n",
    "|--------|--------|\n",
    "| ‚ùå M√©thode lin√©aire | Ne capture pas les non-lin√©arit√©s complexes, contrairement aux mod√®les deep |\n",
    "| ‚ùå Pas de gestion des vues manquantes | Chaque vue est suppos√©e compl√®te et bien align√©e |\n",
    "| ‚ùå Risque de sur-lissage √©vit√© uniquement via un filtrage 1-hop | Ce choix reste rigide dans certains cas |\n",
    "| ‚ùå Pas de m√©canisme d'apprentissage end-to-end avec supervision √©ventuelle | Mod√®le strictement non supervis√© |\n",
    "| ‚ùå Pas de m√©canisme explicite de fusion dynamique | Le poids est fix√© apr√®s initialisation (pas appris pendant optimisation) |\n",
    "\n",
    "## üß† Conclusion\n",
    "LMGEC se positionne comme une **alternative simple, rapide et robuste** aux m√©thodes complexes bas√©es sur GCN ou autoencodeurs. Il est particuli√®rement pertinent dans des contextes contraints en ressources ou n√©cessitant une interpr√©tabilit√© forte.\n",
    "\n",
    "Cependant, pour des cas tr√®s non-lin√©aires ou avec des donn√©es partiellement align√©es, des m√©thodes plus expressives comme les mod√®les √† attention ou graph contrastif peuvent √™tre pr√©f√©r√©es.\n",
    "\n",
    "## üåê R√©capitulatif Synth√©tique\n",
    "\n",
    "| √âl√©ment | D√©scription |\n",
    "|--------|-------------|\n",
    "| **Fusion** | Tardive, lin√©aire, pond√©ration adaptative |\n",
    "| **Hypoth√®ses sur les vues** | M√™mes n≈ìuds, h√©t√©rog√©n√©it√© support√©e |\n",
    "| **Formulation** | Lin√©aire, objectif joint reconstruction + clustering |\n",
    "| **Donn√©es** | Multi-vues topologiques, attributaires ou mixtes |\n",
    "| **M√©triques** | NMI, ARI, ACC, F1 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ce8ac",
   "metadata": {},
   "source": [
    "# Analyse de l‚Äôarticle \"Efficient Graph Convolution for Joint Node Representation Learning and Clustering\" (Fettal et al., WSDM 2022)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Probl√©matique centrale\n",
    "L‚Äôarticle traite le probl√®me de l‚Äô**apprentissage conjoint des repr√©sentations de n≈ìuds et du clustering dans les graphes attribu√©s**. L‚Äôobjectif est d‚Äôam√©liorer la qualit√© du clustering en int√©grant directement l‚Äôobjectif de regroupement dans la phase d‚Äôapprentissage des embeddings, tout en garantissant une **efficacit√© computationnelle**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Type de fusion\n",
    "- **Type :** Interm√©diaire  \n",
    "  La fusion des vues (structure + attributs) se fait via une fonction d‚Äôagr√©gation `agg(A, X) = T·µñX`, c‚Äôest-√†-dire une **fusion interm√©diaire**.\n",
    "  \n",
    "- **Nature :** Lin√©aire  \n",
    "  La fusion est **lin√©aire** : l'agr√©gation est une multiplication de matrices entre une propagation modifi√©e $$T^p$$ et la matrice d'attributs $$X$$.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Pond√©ration des vues\n",
    "- Il n‚Äôy a **pas de pond√©ration explicite des vues**. Le mod√®le utilise une **fusion fixe** de la structure (via le filtre $$T^p$$) et des attributs sans m√©canisme d‚Äôattention ou de softmax.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Hypoth√®ses sur les vues\n",
    "- Les vues **partagent les m√™mes n≈ìuds** (les attributs sont associ√©s aux m√™mes sommets que ceux du graphe).\n",
    "- Les vues peuvent avoir des **topologies diff√©rentes** (on peut appliquer diff√©rentes puissances de propagation $$p$$).\n",
    "- Le mod√®le **ne traite pas explicitement les vues manquantes ou d√©salign√©es**.\n",
    "- Il **r√©duit l‚Äôimpact des vues bruit√©es** indirectement via le filtrage **low-pass** du signal (par normalisation spectrale du filtre).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Formalisme math√©matique du mod√®le\n",
    "\n",
    "### Fonction objectif (Eq. (1)) :\n",
    "$$\n",
    "\\min_{\\theta_1,\\theta_2,G,F} \\|\\text{dec}_{\\theta_2}(\\text{enc}_{\\theta_1}(T^p X)) - T^p X\\|^2 + \\alpha \\|\\text{enc}_{\\theta_1}(T^p X) - GF\\|^2\n",
    "$$\n",
    "\n",
    "- **1er terme :** reconstruction (type autoencodeur lin√©aire)\n",
    "- **2e terme :** r√©gularisation de clustering (k-means sur les embeddings)\n",
    "\n",
    "### Contrainte :\n",
    "$$\n",
    "G \\in \\{0,1\\}^{n \\times k}, \\quad G1_k = 1_n\n",
    "$$\n",
    "(matrice d‚Äôaffectation de clusters : chaque n≈ìud appartient √† un et un seul cluster)\n",
    "\n",
    "### Reformulation (Eq. (5), (6)) :\n",
    "Hypoth√®se que $$W = W_1 = W_2^T$$, et que $$W^T W = I_k$$ (orthogonalit√©), on obtient :\n",
    "\n",
    "$$\n",
    "\\min_{G,F,W} \\|T^p X - GFW^T\\|^2, \\quad \\text{s.c. } G \\in \\{0,1\\}^{n \\times k}, \\, G1_k = 1_n, \\, W^T W = I_k\n",
    "$$\n",
    "\n",
    "### Matrices utilis√©es :\n",
    "- $ T $ : Matrice de propagation normalis√©e (modification de SGC)\n",
    "- $ T^p X $ : signal propag√©\n",
    "- $ G $ : matrice d‚Äôassignation\n",
    "- $ F $ : centro√Ødes\n",
    "- $ W $ : projection orthogonale\n",
    "\n",
    "### Optimisation :\n",
    "- Alternance : mise √† jour de $$G$$, $$F$$, et $$W$$ par descente coordonn√©e\n",
    "- M√©thodes :\n",
    "  - $ F $ : least squares\n",
    "  - $ W $ : Procrustes (SVD)\n",
    "  - $ G $ : k-means\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Donn√©es utilis√©es\n",
    "- Donn√©es **r√©elles** : Cora, Citeseer, Pubmed, Wiki\n",
    "- Les graphes ont les **m√™mes n≈ìuds** dans structure et attributs\n",
    "- **Topologies diff√©rentes** : oui, entre structure (A) et attributs (X)\n",
    "- Pas de **vues manquantes**, toutes les matrices sont compl√®tes\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Exp√©riences\n",
    "- **M√©triques** : Accuracy, NMI, F1-score\n",
    "- **√âtudes d‚Äôablation** :\n",
    "  - Impact de la **matrice de propagation** (comparaison avec normalisations classiques)\n",
    "  - Influence du **param√®tre de propagation** $$p$$\n",
    "- **Robustesse au bruit** : pas explicitement test√©e\n",
    "- **Transductif** : le mod√®le apprend sur le graphe complet (pas inductif)\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Avantages mis en avant\n",
    "- **Formulation simple** et **analytique**\n",
    "- **Complexit√© faible** $$O(p|E|d + tndk)$$\n",
    "- **Performances comparables ou meilleures** que SOTA\n",
    "- **Visualisation claire** via embeddings structur√©s ($$R^2$$ √©lev√©)\n",
    "- **Facilement impl√©mentable** et rapide sur GPU\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Limitations\n",
    "- **Mod√®le transductif**, pas d‚Äôextension inductive\n",
    "- **Param√®tre $$p$$** sensible (propagation) ‚Üí n√©cessite une s√©lection heuristique\n",
    "- Pas de gestion explicite des **vues manquantes** ou des **donn√©es bruit√©es**\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Perspectives d‚Äôam√©lioration\n",
    "- √âtendre √† une **formulation inductive**\n",
    "- Ajouter un **m√©canisme adaptatif pour le poids $$\\alpha$$** entre reconstruction et clustering\n",
    "- Envisager des **versions co-clustering** pour documents/entit√©s bipartites\n",
    "- Ajouter **pond√©ration ou attention** entre structure et attributs\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
