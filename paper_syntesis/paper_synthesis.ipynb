{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0625d030",
   "metadata": {},
   "source": [
    "# Analyse de LMGEC\n",
    "\n",
    "> Article analys√© : **LMGEC: Simultaneous Linear Multi-view Attributed Graph Representation Learning and Clustering**, WSDM 2023.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Probl√©matique centrale  :  \n",
    "LMGEC r√©pond √† la probl√©matique suivante :\n",
    "‚ÄúComment apprendre des repr√©sentations partag√©es et effectuer un clustering efficace sur des graphes multi-vues attribu√©s, tout en garantissant simplicit√©, rapidit√©, et robustesse face √† l‚Äôh√©t√©rog√©n√©it√© des vues ?‚Äù\n",
    ">\n",
    "\n",
    "![Alt text](problem.png)\n",
    "\n",
    "\n",
    "## üìÖ 1. M√©thodologie : quel type de fusion ?\n",
    "\n",
    "LMGEC repose sur une **fusion lin√©aire pond√©e tardive** des vues. \n",
    "\n",
    "- Chaque vue est d'abord **filtr√©e localement** (1-hop) pour lisser les attributs : $$ H_v = S_v X_v $$\n",
    "- Une **pond√©ration adaptative** des vues est appliqu√©e via des poids $$ \\alpha_v $$, calcul√©s dynamiquement.\n",
    "- Les embeddings obtenus sont projet√©s et **fusionn√©s dans un espace commun** pour effectuer le clustering.\n",
    "\n",
    "> **Type de fusion :** tardive + pond√©ration adaptative (soft fusion)\n",
    "\n",
    "---\n",
    "\n",
    "## üîç 2. Hypoth√®ses sur les vues\n",
    "\n",
    "- ‚úÖ M√™mes n≈ìuds dans toutes les vues\n",
    "- ‚úÖ Vues potentiellement **tr√®s h√©t√©rog√®nes** (topologies ou attributs)\n",
    "- ‚ùå Pas de traitement sp√©cial des vues **manquantes** ou d√©salign√©es\n",
    "- ‚úÖ Le mod√®le peut att√©nuer les vues bruit√©es via la pond√©ration $$ \\alpha_v $$\n",
    "\n",
    "---\n",
    "\n",
    "## üìä 3. Mod√®les math√©matiques\n",
    "\n",
    "- **Filtrage de chaque vue :** $$ H_v = S_v X_v $$, o√π $$ S_v = \\tilde{D}^{-1} (\\tilde{A}_v) $$ avec self-loops.\n",
    "\n",
    "- **Objectif :**\n",
    "$$\n",
    "\\min_{G, F, W_1,\\dots,W_V} \\sum_{v=1}^{V} \\alpha_v \\| H_v - G F W_v^\\top \\|^2\n",
    "$$\n",
    "Avec :\n",
    "  - $ G \\in \\{0,1\\}^{n \\times k} $ : clustering (soft or hard)\n",
    "  - $ W_v \\in \\mathbb{R}^{d \\times f} $, $ W_v W_v^\\top = I $\n",
    "\n",
    "- **Pond√©ration des vues :**\n",
    "$$\n",
    "\\alpha_v = \\text{softmax}\\left(-\\frac{I_v}{\\tau}\\right), \\quad I_v = \\| H_v - G_v F_v \\|\n",
    "$$\n",
    "\n",
    "> Optimisation par **Bloc Coordinate Descent**\n",
    "\n",
    "> \n",
    "![Alt text](recon.png)\n",
    "---\n",
    "\n",
    "## üìà 4. Types de donn√©es utilis√©es\n",
    "\n",
    "- **Topologies diff√©rentes, m√™mes features :** ACM, DBLP, IMDB\n",
    "- **M√™mes topologies, features diff√©rentes :** Amazon Photos\n",
    "- **Topologies + features diff√©rentes :** Wiki\n",
    "\n",
    "> LMGEC couvre **tous les cas multi-vues usuels**\n",
    "> \n",
    "![Alt text](topo.png)\n",
    "---\n",
    "\n",
    "## üìä 5. M√©triques d‚Äô√©valuation\n",
    "\n",
    "Les performances sont mesur√©es avec 4 m√©triques standards en clustering :\n",
    "\n",
    "| M√©trique | Description |\n",
    "|----------|-------------|\n",
    "| **NMI** | Normalized Mutual Information |\n",
    "| **ARI** | Adjusted Rand Index |\n",
    "| **ACC** | Accuracy (appliqu√©e au clustering) |\n",
    "| **F1-score** | Pr√©cision + rappel |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# üìä M√©triques d‚Äô√©valuation pour le clustering non supervis√©\n",
    "\n",
    "Lorsque j‚Äô√©value un algorithme de clustering (comme LMGEC), je dois utiliser des m√©triques quantitatives pour mesurer √† quel point les clusters trouv√©s sont coh√©rents avec les classes r√©elles (si elles sont disponibles). Voici les principales m√©triques utilis√©es :\n",
    "\n",
    "---\n",
    "\n",
    "| **M√©trique** | **Nom complet** | **Ce qu‚Äôelle mesure** | **Valeurs typiques** |\n",
    "|--------------|------------------|------------------------|-----------------------|\n",
    "| **NMI** | Normalized Mutual Information | Le degr√© de similarit√© entre les clusters pr√©dits et les vraies classes, bas√© sur l‚Äôinformation partag√©e | Entre 0 (aucune info) et 1 (parfait) |\n",
    "| **ARI** | Adjusted Rand Index | √Ä quel point deux partitions (clusters vs classes) sont en accord, corrig√© pour le hasard | Entre -1 (pire que hasard) et 1 (parfait accord) |\n",
    "| **ACC** | Clustering Accuracy | Pourcentage d‚Äôexemples correctement class√©s, apr√®s r√©assignation optimale des labels | Entre 0 et 1 (ou 0% √† 100%) |\n",
    "| **F1-score** | F1 Measure (harmonique pr√©cision / rappel) | Moyenne harmonique entre la pr√©cision (exactitude des clusters) et le rappel (couverture des vraies classes) | Entre 0 et 1 |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† D√©tails compl√©mentaires\n",
    "\n",
    "### üîπ NMI (Normalized Mutual Information)\n",
    "- Formule :\n",
    "$$\n",
    "\\text{NMI}(Y, C) = \\frac{2 \\, I(Y; C)}{H(Y) + H(C)}\n",
    "$$\n",
    "avec \\( I(Y; C) \\) l'information mutuelle entre la partition vraie \\( Y \\) et pr√©dite \\( C \\).\n",
    "- Avantage : **invariante aux permutations de labels**.\n",
    "\n",
    "### üîπ ARI (Adjusted Rand Index)\n",
    "- Tient compte du **nombre de paires** correctement regroup√©es ou s√©par√©es.\n",
    "- Corrige le **Rand Index** pour compenser le regroupement al√©atoire.\n",
    "- Avantage : bonne robustesse m√™me avec des d√©s√©quilibres de classe.\n",
    "\n",
    "### üîπ ACC (Accuracy)\n",
    "- Mesure directe et intuitive.\n",
    "- Requiert une **permutation optimale des clusters** (car les labels peuvent √™tre invers√©s).\n",
    "\n",
    "### üîπ F1-score\n",
    "- Combine deux notions :\n",
    "  - **Pr√©cision** : parmi les √©l√©ments du cluster, combien sont bien class√©s ?\n",
    "  - **Rappel** : parmi les vrais √©l√©ments d‚Äôune classe, combien sont captur√©s ?\n",
    "- L‚Äô√©quilibre entre les deux est utile surtout pour les classes d√©s√©quilibr√©es.\n",
    "\n",
    "---\n",
    "\n",
    "> En r√©sum√© : utiliser plusieurs m√©triques permet d‚Äôavoir une vue plus compl√®te de la qualit√© du clustering. NMI et ARI sont les plus stables pour comparer les m√©thodes, tandis que ACC et F1 donnent une interpr√©tation plus intuitive des performances.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# üéØ Contribution originale et √©valuation critique de LMGEC\n",
    "\n",
    "## üß© Quelle est la **contribution originale** du papier ?\n",
    "\n",
    "L‚Äôarticle propose **LMGEC**, un mod√®le lin√©aire simple et efficace pour :\n",
    "\n",
    "- R√©aliser **simultan√©ment** l‚Äôapprentissage de repr√©sentation et le clustering sur des graphes multi-vues attribu√©s.\n",
    "- Offrir une **formulation unifi√©e** int√©grant :\n",
    "  - une √©tape de **propagation locale (1-hop)** par un filtre de graphe lin√©aire,\n",
    "  - un **m√©canisme de pond√©ration adaptative des vues** (softmax sur l‚Äôinertie),\n",
    "  - un objectif combin√© de **reconstruction + clustering**.\n",
    "- √ätre **g√©n√©rique**, applicable √† :\n",
    "  - plusieurs graphes avec une m√™me matrice de features,\n",
    "  - plusieurs matrices de features sur un seul graphe,\n",
    "  - ou un m√©lange des deux (cas du dataset Wiki).\n",
    "- √ätre **beaucoup plus rapide** que les mod√®les existants tout en offrant des performances comparables, voire meilleures.\n",
    "- Fournir une **analyse math√©matique et exp√©rimentale approfondie** ainsi que le **code open-source**.\n",
    "\n",
    "## ‚úÖ **Points forts** de LMGEC\n",
    "\n",
    "| Atout | D√©tail |\n",
    "|-------|--------|\n",
    "| ‚úÖ Simplicit√© | Formulation lin√©aire claire et interpr√©table |\n",
    "| ‚úÖ Efficacit√© | Temps d'entra√Ænement **jusqu'√† 10√ó plus rapide** que les m√©thodes GCN ou autoencoder |\n",
    "| ‚úÖ Robustesse | Capacit√© √† ignorer les vues peu informatives via le m√©canisme d'attention/inertie |\n",
    "| ‚úÖ G√©n√©ralit√© | Supporte diff√©rents types de graphes multi-vues sans contraintes |\n",
    "| ‚úÖ Formulation unifi√©e | Apprentissage de repr√©sentation + clustering dans un m√™me objectif |\n",
    "| ‚úÖ Reproductibilit√© | Code disponible en open-source et r√©sultats d√©taill√©s sur 5 benchmarks |\n",
    "\n",
    "## ‚ö†Ô∏è **Limites** de LMGEC\n",
    "\n",
    "| Limite | D√©tail |\n",
    "|--------|--------|\n",
    "| ‚ùå M√©thode lin√©aire | Ne capture pas les non-lin√©arit√©s complexes, contrairement aux mod√®les deep |\n",
    "| ‚ùå Pas de gestion des vues manquantes | Chaque vue est suppos√©e compl√®te et bien align√©e |\n",
    "| ‚ùå Risque de sur-lissage √©vit√© uniquement via un filtrage 1-hop | Ce choix reste rigide dans certains cas |\n",
    "| ‚ùå Pas de m√©canisme d'apprentissage end-to-end avec supervision √©ventuelle | Mod√®le strictement non supervis√© |\n",
    "| ‚ùå Pas de m√©canisme explicite de fusion dynamique | Le poids est fix√© apr√®s initialisation (pas appris pendant optimisation) |\n",
    "\n",
    "## üß† Conclusion\n",
    "LMGEC se positionne comme une **alternative simple, rapide et robuste** aux m√©thodes complexes bas√©es sur GCN ou autoencodeurs. Il est particuli√®rement pertinent dans des contextes contraints en ressources ou n√©cessitant une interpr√©tabilit√© forte.\n",
    "\n",
    "Cependant, pour des cas tr√®s non-lin√©aires ou avec des donn√©es partiellement align√©es, des m√©thodes plus expressives comme les mod√®les √† attention ou graph contrastif peuvent √™tre pr√©f√©r√©es.\n",
    "\n",
    "## üåê R√©capitulatif Synth√©tique\n",
    "\n",
    "| √âl√©ment | D√©scription |\n",
    "|--------|-------------|\n",
    "| **Fusion** | Tardive, lin√©aire, pond√©ration adaptative |\n",
    "| **Hypoth√®ses sur les vues** | M√™mes n≈ìuds, h√©t√©rog√©n√©it√© support√©e |\n",
    "| **Formulation** | Lin√©aire, objectif joint reconstruction + clustering |\n",
    "| **Donn√©es** | Multi-vues topologiques, attributaires ou mixtes |\n",
    "| **M√©triques** | NMI, ARI, ACC, F1 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef69d09",
   "metadata": {},
   "source": [
    "# Analyse de GCC\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Quelle est la probl√©matique centrale √† laquelle r√©pond l‚Äôarticle ?\n",
    "\n",
    "L‚Äôarticle traite **du clustering de graphes attribu√©s**, o√π chaque n≈ìud a des attributs. Le d√©fi est de faire **l‚Äôapprentissage de repr√©sentations et le clustering conjointement**, plut√¥t que de mani√®re s√©quentielle (tandem). La m√©thode vise √† produire des **repr√©sentations adapt√©es √† la structure de clusters**, tout en √©tant **efficace computationnellement**.\n",
    "\n",
    "- üß† **D√©fi principal :** clustering efficace et conjoint avec l‚Äôapprentissage de repr√©sentations dans les graphes attribu√©s, avec une complexit√© r√©duite.\n",
    "\n",
    "![Q1](2-1.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Quel type de fusion est utilis√© pour combiner les diff√©rentes vues ?\n",
    "\n",
    "M√™me si GCC ne traite pas explicitement de donn√©es multivues, il applique une fusion **pr√©coce lin√©aire** entre la topologie $A$ et les attributs $X$, via une op√©ration d‚Äôagr√©gation :\n",
    "\n",
    "- Type de fusion : **pr√©coce**\n",
    "- Nature de la fusion : **lin√©aire**\n",
    "- Agr√©gation : $$ \\text{agg}(A, X) = T^p X $$\n",
    "\n",
    "![Q2](2-2.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Y a-t-il une pond√©ration des vues ?\n",
    "\n",
    "GCC ne traite qu‚Äôun seul graphe, donc **pas de pond√©ration explicite entre vues**. Toutefois, le param√®tre $p$ contr√¥le combien de \"hops\" sont pris en compte, ce qui agit comme une **pond√©ration spatiale implicite**.\n",
    "\n",
    "- ‚ùå Pas de softmax, attention ou pond√©ration dynamique.\n",
    "- ‚úÖ Influence implicite via $p$.\n",
    "\n",
    "![Q3](2-3.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Quelles sont les hypoth√®ses faites sur les vues ?\n",
    "\n",
    "M√™me dans un cadre monovue, GCC repose sur des hypoth√®ses implicites :\n",
    "\n",
    "- ‚úÖ Les vues (topologie + attributs) **partagent les m√™mes n≈ìuds**.\n",
    "- ‚ùå Ne supporte pas de **vues h√©t√©rog√®nes** ou **d√©salign√©es**.\n",
    "- ‚ùå Ne g√®re pas les **vues manquantes**.\n",
    "- ‚ùå Pas de m√©canisme explicite contre le **bruit inter-vue**.\n",
    "\n",
    "![Q4](2-4.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Quel est le formalisme math√©matique du mod√®le ?\n",
    "\n",
    "Le mod√®le repose sur une **optimisation conjointe** d‚Äôune perte de reconstruction et d‚Äôune r√©gularisation de clustering :\n",
    "\n",
    "### Fonction objectif principale :\n",
    "$$\n",
    "\\min_{G, F, W} \\| T^p X - T^p X W W^\\top \\|^2 + \\alpha \\| T^p X W - G F \\|^2\n",
    "$$\n",
    "\n",
    "### Avec contraintes :\n",
    "- $W^\\top W = I_k$\n",
    "- $G \\in \\{0, 1\\}^{n \\times k}$ et $G \\mathbf{1}_k = \\mathbf{1}_n$\n",
    "\n",
    "On peut reformuler en :\n",
    "$$\n",
    "\\min_{G, F, W} \\| T^p X - G F W^\\top \\|^2\n",
    "$$\n",
    "\n",
    "### M√©thode d‚Äôoptimisation :\n",
    "- Alternance :\n",
    "  - $F \\leftarrow (G^\\top G)^{-1} G^\\top T^p X W$\n",
    "  - $W \\leftarrow \\text{SVD}( (T^p X)^\\top G F )$\n",
    "  - $G \\leftarrow \\arg\\min_j \\| (T^p X W)_i - f_j \\|^2$\n",
    "\n",
    "![Q5](2-5.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Quel type de donn√©es est utilis√© pour l‚Äô√©valuation ?\n",
    "\n",
    "Les jeux de donn√©es sont **r√©els**, avec une topologie fixe et des attributs diff√©rents selon les graphes :\n",
    "\n",
    "| Dataset   | # Nodes | # Edges | # Features | # Classes |\n",
    "|-----------|---------|---------|------------|-----------|\n",
    "| Citeseer  | 3327    | 4732    | 3703       | 6         |\n",
    "| Cora      | 2708    | 5429    | 1433       | 7         |\n",
    "| Pubmed    | 19717   | 44338   | 500        | 3         |\n",
    "| Wiki      | 2405    | 17981   | 4973       | 17        |\n",
    "\n",
    "- ‚úÖ M√™me n≈ìuds, topologies fixes\n",
    "- ‚úÖ Attributs textuels\n",
    "- ‚úÖ Jeux de donn√©es enti√®rement r√©els\n",
    ">\n",
    "![Q5](2-6.png)\n",
    "---\n",
    "\n",
    "## 7. Quelles exp√©riences sont men√©es pour valider le mod√®le ?\n",
    "\n",
    "- **Comparaison avec les baselines** (S2GC, GIC, DCN, etc.) sur ACC, NMI, F1\n",
    "- **Ablation sur la normalisation** utilis√©e pour $T$\n",
    "- **Visualisation des embeddings** via t-SNE + analyse R¬≤\n",
    "- **Temps d‚Äôex√©cution** compar√© aux autres m√©thodes (AGE, S2GC‚Ä¶)\n",
    "- **S√©lection automatique du param√®tre** $p$\n",
    "\n",
    "> Le mod√®le est **transductif**, pas inductif.\n",
    "> \n",
    "![Q5](2-7.png)\n",
    "---\n",
    "\n",
    "## 8. Quels sont les avantages mis en avant par les auteurs ?\n",
    "\n",
    "- ‚úÖ **Simplicit√©** (mod√®le lin√©aire, formulation analytique)\n",
    "- ‚úÖ **Rapidit√©** (SVD, closed-form updates)\n",
    "- ‚úÖ **Optimisation d√©terministe** (descente garantie)\n",
    "- ‚úÖ **Repr√©sentations de qualit√©** pour clustering et visualisation\n",
    "- ‚úÖ **Scalabilit√©** prouv√©e sur des graphes de grande taille (ex : Pubmed)\n",
    ">\n",
    "![Q5](2-8.png)\n",
    "---\n",
    "\n",
    "## 9. Quelles sont les limitations ou points faibles du mod√®le ?\n",
    "\n",
    "- ‚ùå Pas de version **inductive**\n",
    "- ‚ùå Sensible √† l‚Äô**initialisation**\n",
    "- ‚ùå $p$ doit √™tre **choisi heuristiquement**\n",
    "- ‚ùå Ne supporte pas **vues multiples**, bruit√©es, ou **manquantes**\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Quelles sont les perspectives d‚Äôam√©lioration discut√©es ?\n",
    "\n",
    "- üîÑ Optimisation du **coefficient $\\alpha$** entre reconstruction et clustering\n",
    "- üîÆ D√©veloppement d‚Äôune version **inductive**\n",
    "- üß© Extension vers le **co-clustering** (noeuds + attributs)\n",
    "- üåê Adaptation plus fine du **param√®tre $p$**\n",
    "\n",
    "---\n",
    "\n",
    "**Fin du rapport.**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
