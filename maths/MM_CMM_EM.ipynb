{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mod√®les de M√©lange et Algorithmes EM : Th√©orie Fondamentale et Applications Avanc√©es\n",
    "\n",
    "## 1. Introduction aux Mod√®les de M√©lange (Mixture Models)\n",
    "Les mod√®les de m√©lange sont une famille de mod√®les probabilistes qui supposent que les donn√©es observ√©es sont g√©n√©r√©es √† partir d‚Äôune combinaison (m√©lange) de plusieurs distributions sous-jacentes. Ils sont particuli√®rement utiles pour :\n",
    "\n",
    "- La classification non supervis√©e (clustering)\n",
    "- L‚Äôestimation de densit√© (density estimation)\n",
    "- La mod√©lisation de donn√©es h√©t√©rog√®nes\n",
    "\n",
    "### 1.1 Formulation Math√©matique\n",
    "Un mod√®le de m√©lange s‚Äô√©crit sous la forme g√©n√©rale :\n",
    "\n",
    "$$\n",
    "p(x \\mid \\theta) = \\sum_{k=1}^{K} \\pi_k \\, p(x \\mid \\theta_k)\n",
    "$$\n",
    "\n",
    "o√π :\n",
    "\n",
    "- $K$ = nombre de composantes du m√©lange\n",
    "- $\\pi_k$ = poids du $k$-i√®me composant ($\\pi_k \\geq 0$, $\\sum_{k=1}^{K} \\pi_k = 1$)\n",
    "- $p(x \\mid \\theta_k)$ = distribution de probabilit√© du $k$-i√®me composant (ex : Gaussienne, Poisson, etc.)\n",
    "- $\\theta = \\{(\\pi_k, \\theta_k)\\}_{k=1}^{K}$ = param√®tres du mod√®le\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.2 Exemple : Mod√®le de M√©lange Gaussien (GMM)\n",
    "Si chaque composante est une loi normale multivari√©e, alors :\n",
    "\n",
    "$$\n",
    "p(x \\mid \\theta_k) = N(x \\mid \\mu_k, \\Sigma_k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}} \\exp\\left( -\\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k) \\right)\n",
    "$$\n",
    "\n",
    "o√π :\n",
    "\n",
    "- $\\mu_k$ = vecteur moyenne\n",
    "- $\\Sigma_k$ = matrice de covariance\n",
    "- $d$ = dimension des donn√©es\n",
    "\n",
    "\n",
    "\n",
    "# Loi Normale Multivari√©e : D√©monstration\n",
    "\n",
    "## üìå Formule √† d√©montrer\n",
    "Soit $x \\in \\mathbb{R}^p$ une variable al√©atoire suivant une loi normale multivari√©e de moyenne $\\mu$ et de matrice de covariance $\\Sigma$.\n",
    "\n",
    "**Densit√© de probabilit√© :**\n",
    "$$\n",
    "f(x \\mid \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu)\\right)\n",
    "$$\n",
    "\n",
    "## 1Ô∏è‚É£ Cas univari√© (rappel)\n",
    "Pour $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ :\n",
    "$$\n",
    "f(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "Le terme $\\frac{(x-\\mu)^2}{\\sigma^2}$ repr√©sente la distance normalis√©e.\n",
    "\n",
    "## 2Ô∏è‚É£ Extension multivari√©e\n",
    "Pour un vecteur al√©atoire $x \\in \\mathbb{R}^p$ :\n",
    "\n",
    "- $\\mu \\in \\mathbb{R}^p$ : vecteur moyenne\n",
    "- $\\Sigma \\in \\mathbb{R}^{p√óp}$ : matrice de covariance\n",
    "\n",
    "**G√©n√©ralisations :**\n",
    "1. $\\sigma^2$ ‚Üí $\\Sigma$ (matrice de covariance)\n",
    "2. $\\frac{(x-\\mu)^2}{\\sigma^2}$ ‚Üí $(x-\\mu)^T \\Sigma^{-1} (x-\\mu)$ (distance de Mahalanobis)\n",
    "3. Normalisation : $\\frac{1}{\\sqrt{2\\pi\\sigma^2}}$ ‚Üí $\\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}}$\n",
    "\n",
    "## 3Ô∏è‚É£ D√©monstration compl√®te\n",
    "\n",
    "### √âtape 1 : Fonction caract√©ristique\n",
    "Pour $X \\sim \\mathcal{N}(\\mu, \\Sigma)$ :\n",
    "$$\n",
    "\\varphi_x(t) = \\exp\\left(i t^T \\mu - \\frac{1}{2} t^T \\Sigma t\\right)\n",
    "$$\n",
    "\n",
    "### √âtape 2 : Transform√©e de Fourier inverse\n",
    "La densit√© s'obtient par :\n",
    "$$\n",
    "f(x) = \\frac{1}{(2\\pi)^p} \\int_{\\mathbb{R}^p} \\exp(-i t^T x) \\varphi_x(t) dt\n",
    "$$\n",
    "\n",
    "En r√©solvant cette int√©grale gaussienne, on retrouve la formule :\n",
    "$$\n",
    "f(x \\mid \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu)\\right)\n",
    "$$\n",
    "\n",
    "## 4Ô∏è‚É£ Interpr√©tation\n",
    "\n",
    "### Terme de normalisation\n",
    "- Garantit $\\int_{\\mathbb{R}^p} f(x)dx = 1$\n",
    "- $|\\Sigma|^{1/2}$ ajuste le volume dans $\\mathbb{R}^p$\n",
    "\n",
    "### Terme exponentiel\n",
    "- Mesure la distance de Mahalanobis\n",
    "- Tient compte des corr√©lations entre variables\n",
    "- Si $\\Sigma$ est diagonale : variables ind√©pendantes\n",
    "\n",
    "## Tableau r√©capitulatif\n",
    "\n",
    "| Cas univari√© | Cas multivari√© |\n",
    "|--------------|----------------|\n",
    "| $\\sigma^2$ | $\\Sigma$ |\n",
    "| $\\frac{(x-\\mu)^2}{\\sigma^2}$ | $(x-\\mu)^T \\Sigma^{-1} (x-\\mu)$ |\n",
    "| $\\frac{1}{\\sqrt{2\\pi\\sigma^2}}$ | $\\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}$ |\n",
    "\n",
    "**Propri√©t√© cl√© :** Quand $\\Sigma$ est diagonale, on retrouve le produit de $p$ lois normales ind√©pendantes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Mod√®les de M√©lange Gaussien\n",
    "\n",
    "## 1Ô∏è‚É£ Structure du mod√®le\n",
    "Le param√®tre global $\\Phi$ contient :\n",
    "- **Proportions** : $\\pi_1, \\ldots, \\pi_g$ (probabilit√©s d'appartenance aux groupes)\n",
    "- **Param√®tres** : $\\theta_k = (\\mu_k, \\Sigma_k)$ pour chaque composante $k$\n",
    "\n",
    "**Densit√© du m√©lange** :\n",
    "$$\n",
    "\\varphi(x, \\Phi) = \\sum_{k=1}^g \\pi_k f(x, \\theta_k)\n",
    "$$\n",
    "o√π $f(x, \\theta_k)$ est la densit√© $\\mathcal{N}(\\mu_k, \\Sigma_k)$.\n",
    "\n",
    "## 2Ô∏è‚É£ Interpr√©tation probabiliste\n",
    "### Formulation jointe :\n",
    "$$\n",
    "\\varphi(x, \\Phi) = \\sum_{k=1}^g P(\\theta_k) P(x \\mid \\theta_k) = P\\left(\\bigcup_{k=1}^g \\{(x, \\theta_k)\\}\\right)\n",
    "$$\n",
    "\n",
    "### Signification :\n",
    "- $P(x, \\theta_k)$ : Probabilit√© conjointe observation-groupe\n",
    "- La somme repr√©sente la probabilit√© totale sur tous les groupes\n",
    "\n",
    "## 3Ô∏è‚É£ Estimation par maximum de vraisemblance\n",
    "**Fonction de vraisemblance** (pour $p$ observations) :\n",
    "$$\n",
    "\\mathcal{L}(x; \\Phi) = \\sum_{i=1}^p \\log \\left( \\sum_{k=1}^g \\pi_k f(x_i, \\theta_k) \\right)\n",
    "$$\n",
    "\n",
    "**Processus d'optimisation** :\n",
    "1. Calculer $f(x_i, \\theta_k)$ pour chaque observation et composante\n",
    "2. Pond√©rer par $\\pi_k$\n",
    "3. Maximiser $\\mathcal{L}$ via EM (Expectation-Maximization)\n",
    "\n",
    "## 4Ô∏è‚É£ Attribution des classes\n",
    "**Probabilit√© a posteriori** (r√®gle de Bayes) :\n",
    "$$\n",
    "P(x_i \\in G_k \\mid x_i) = \\frac{\\pi_k f(x_i, \\theta_k)}{\\sum_{\\ell=1}^g \\pi_\\ell f(x_i, \\theta_\\ell)}\n",
    "$$\n",
    "\n",
    "**D√©cision** :\n",
    "Attribuer $x_i$ au groupe $k^* = \\arg\\max_k P(x_i \\in G_k \\mid x_i)$\n",
    "\n",
    "## Tableau r√©capitulatif\n",
    "| Concept | Formule |\n",
    "|---------|---------|\n",
    "| Densit√© | $\\sum \\pi_k \\mathcal{N}(\\mu_k, \\Sigma_k)$ |\n",
    "| Vraisemblance | $\\sum \\log(\\sum \\pi_k f(x_i, \\theta_k))$ |\n",
    "| Bayes | $\\frac{\\pi_k f_k}{\\sum \\pi_\\ell f_\\ell}$ |\n",
    "\n",
    "## Propri√©t√©s cl√©s\n",
    "- Flexibilit√© pour mod√©liser des distributions complexes\n",
    "- Estimation it√©rative via algorithme EM\n",
    "- Requiert de sp√©cifier $g$ (nombre de composantes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2. Algorithme EM (Expectation-Maximization)\n",
    "L‚Äôalgorithme EM est une m√©thode it√©rative pour estimer les param√®tres $$\\theta$$ d‚Äôun mod√®le de m√©lange lorsque certaines variables sont latentes (non observ√©es).\n",
    "\n",
    "### 2.1 Variables Latentes et Donn√©es Compl√®tes\n",
    "Donn√©es observ√©es : \n",
    "$$ X = \\{x_1, \\dots, x_N\\} $$\n",
    "\n",
    "Donn√©es latentes : \n",
    "\n",
    "$Z = \\{z_1, \\dots, z_N\\}$, o√π $z_{nk} = 1$ si $x_n$ provient de la composante $k$, sinon $0$.\n",
    "\n",
    "La vraisemblance compl√®te s'√©crit :\n",
    "\n",
    "$$\n",
    "p(X, Z \\mid \\theta) = \\prod_{n=1}^{N} \\prod_{k=1}^{K} \\left[\\pi_k \\, p(x_n \\mid \\theta_k)\\right]^{z_{nk}}\n",
    "$$\n",
    "\n",
    "### 2.2 √âtapes de l‚ÄôAlgorithme EM\n",
    "#### (E-Step) : Calcul des Responsabilit√©s\n",
    "On calcule l‚Äôesp√©rance des variables latentes $$Z$$ √©tant donn√© les param√®tres courants $$\\theta^{(t)}$$ :\n",
    "\n",
    "$$\n",
    "\\gamma_{nk} = \\mathbb{E}[z_{nk} \\mid x_n, \\theta^{(t)}] = \\frac{\\pi_k^{(t)} p(x_n \\mid \\theta_k^{(t)})}{\\sum_{j=1}^{K} \\pi_j^{(t)} p(x_n \\mid \\theta_j^{(t)})}\n",
    "$$\n",
    "\n",
    "#### (M-Step) : Maximisation de la Vraisemblance\n",
    "On met √† jour les param√®tres en maximisant l‚Äôesp√©rance de la log-vraisemblance compl√®te :\n",
    "\n",
    "$$\n",
    "\\theta^{(t+1)} = \\arg \\max_{\\theta} \\mathbb{E}[ \\log p(X, Z \\mid \\theta) \\mid X, \\theta^{(t)}]\n",
    "$$\n",
    "\n",
    "Pour un GMM, les mises √† jour sont explicites :\n",
    "\n",
    "- Poids des composantes :\n",
    "\n",
    "$$\n",
    "\\pi_k^{(t+1)} = \\frac{1}{N} \\sum_{n=1}^{N} \\gamma_{nk}\n",
    "$$\n",
    "\n",
    "- Moyennes :\n",
    "\n",
    "$$\n",
    "\\mu_k^{(t+1)} = \\frac{\\sum_{n=1}^{N} \\gamma_{nk} x_n}{\\sum_{n=1}^{N} \\gamma_{nk}}\n",
    "$$\n",
    "\n",
    "- Matrices de covariance :\n",
    "\n",
    "$$\n",
    "\\Sigma_k^{(t+1)} = \\frac{\\sum_{n=1}^{N} \\gamma_{nk} (x_n - \\mu_k^{(t+1)})(x_n - \\mu_k^{(t+1)})^T}{\\sum_{n=1}^{N} \\gamma_{nk}}\n",
    "$$\n",
    "\n",
    "## 3. Application : Clustering avec les CMM (Conditional Mixture Models)\n",
    "Les CMM sont une variante des mod√®les de m√©lange o√π chaque composante est centr√©e sur un point de donn√©es (exemplaire).\n",
    "\n",
    "### 3.1 Formulation du Probl√®me\n",
    "Soit un dataset $$ X = \\{x_1, \\dots, x_N\\} $$, la distribution du m√©lange est :\n",
    "\n",
    "$$\n",
    "Q(x) = \\sum_{j=1}^{N} q_j f_j(x)\n",
    "$$\n",
    "\n",
    "o√π :\n",
    "\n",
    "- $f_j(x) = C_\\phi(x) \\exp\\left(-\\beta \\, d_\\phi(x, x_j)\\right)$ (distribution exponentielle)\n",
    "- $d_\\phi$ = divergence de Bregman (ex : distance euclidienne $\\|x - x_j\\|_2$)\n",
    "- $q_j$ = probabilit√© a priori du $j$-i√®me exemplaire\n",
    "\n",
    "### 3.2 Maximisation de la Log-Vraisemblance\n",
    "La log-vraisemblance s‚Äô√©crit :\n",
    "\n",
    "$$\n",
    "L(X; \\{q_j\\}) = \\frac{1}{N} \\sum_{i=1}^{N} \\log \\left( \\sum_{j=1}^{N} q_j \\exp\\left( -\\beta d_\\phi(x_i, x_j) \\right) \\right) + \\text{const}\n",
    "$$\n",
    "\n",
    "### 3.3 Minimisation de la Divergence KL\n",
    "Le probl√®me est √©quivalent √† minimiser :\n",
    "\n",
    "$$\n",
    "D(P \\parallel Q) = -L + \\text{const}\n",
    "$$\n",
    "\n",
    "o√π $$ P $$ est la distribution empirique.\n",
    "\n",
    "### 3.4 Mise √† Jour des Param√®tres\n",
    "La r√®gle de mise √† jour EM pour $$ q_j $$ est :\n",
    "\n",
    "$$\n",
    "q_j^{(t+1)} = q_j^{(t)} \\frac{\\sum_{i=1}^{N} P(x_i) f_j(x_i)}{\\sum_{j'} q_{j'}^{(t)} f_{j'}(x_i)}\n",
    "$$\n",
    "\n",
    "## 4. √âtude de Cas : Segmentation d‚ÄôImages par GMM\n",
    "### 4.1 Probl√®me\n",
    "Segmenter une image en 3 r√©gions (fond, objet, bordure) en utilisant un GMM sur les pixels.\n",
    "\n",
    "### 4.2 Mod√©lisation\n",
    "Chaque pixel $$ x_n $$ est repr√©sent√© par sa couleur (RGB).\n",
    "\n",
    "On utilise un GMM √† 3 composantes.\n",
    "\n",
    "### 4.3 Algorithme EM Appliqu√©\n",
    "**Initialisation** :\n",
    "\n",
    "- $$ \\pi_k = \\frac{1}{3} $$  \n",
    "- $ \\mu_k $ = centres issus du K-means  \n",
    "- $ \\Sigma_k $ = matrices identit√©\n",
    "\n",
    "**E-Step** :  \n",
    "Calculer $$ \\gamma_{nk} $$ pour chaque pixel.\n",
    "\n",
    "**M-Step** :  \n",
    "Mettre √† jour $$ \\pi_k, \\mu_k, \\Sigma_k $$.\n",
    "\n",
    "**Convergence** :  \n",
    "Apr√®s ~20 it√©rations, les pixels sont class√©s en 3 clusters.\n",
    "\n",
    "### 4.4 R√©sultats\n",
    "- Cluster 1 = Fond (couleur uniforme)\n",
    "- Cluster 2 = Objet principal\n",
    "- Cluster 3 = Contours\n",
    "\n",
    "## 5. Conclusion et Perspectives\n",
    "Les mod√®les de m√©lange offrent une approche flexible pour mod√©liser des donn√©es complexes.\n",
    "\n",
    "L‚Äôalgorithme EM permet une estimation efficace des param√®tres m√™me avec des donn√©es manquantes.\n",
    "\n",
    "Les CMM √©tendent cette approche en utilisant des exemplaires, utile pour le clustering.\n",
    "\n",
    "**Applications avanc√©es** :\n",
    "\n",
    "- Reconnaissance vocale (HMM)\n",
    "- Imagerie m√©dicale (segmentation)\n",
    "- Recommandation (collaborative filtering)\n",
    "\n",
    "## R√©f√©rences :\n",
    "\n",
    "- Bishop, *Pattern Recognition and Machine Learning* (2006)\n",
    "- Dempster et al., *Maximum Likelihood from Incomplete Data via the EM Algorithm* (1977)\n",
    "- [Paper original sur les CMM]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
