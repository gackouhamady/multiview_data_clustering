{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèles de Mélange et Algorithmes EM : Théorie Fondamentale et Applications Avancées\n",
    "\n",
    "## 1. Introduction aux Modèles de Mélange (Mixture Models)\n",
    "Les modèles de mélange sont une famille de modèles probabilistes qui supposent que les données observées sont générées à partir d’une combinaison (mélange) de plusieurs distributions sous-jacentes. Ils sont particulièrement utiles pour :\n",
    "\n",
    "- La classification non supervisée (clustering)\n",
    "- L’estimation de densité (density estimation)\n",
    "- La modélisation de données hétérogènes\n",
    "\n",
    "### 1.1 Formulation Mathématique\n",
    "Un modèle de mélange s’écrit sous la forme générale :\n",
    "\n",
    "$$\n",
    "p(x \\mid \\theta) = \\sum_{k=1}^{K} \\pi_k \\, p(x \\mid \\theta_k)\n",
    "$$\n",
    "\n",
    "où :\n",
    "\n",
    "- $K$ = nombre de composantes du mélange\n",
    "- $\\pi_k$ = poids du $k$-ième composant ($\\pi_k \\geq 0$, $\\sum_{k=1}^{K} \\pi_k = 1$)\n",
    "- $p(x \\mid \\theta_k)$ = distribution de probabilité du $k$-ième composant (ex : Gaussienne, Poisson, etc.)\n",
    "- $\\theta = \\{(\\pi_k, \\theta_k)\\}_{k=1}^{K}$ = paramètres du modèle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.2 Exemple : Modèle de Mélange Gaussien (GMM)\n",
    "Si chaque composante est une loi normale multivariée, alors :\n",
    "\n",
    "$$\n",
    "p(x \\mid \\theta_k) = N(x \\mid \\mu_k, \\Sigma_k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}} \\exp\\left( -\\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k) \\right)\n",
    "$$\n",
    "\n",
    "où :\n",
    "\n",
    "- $\\mu_k$ = vecteur moyenne\n",
    "- $\\Sigma_k$ = matrice de covariance\n",
    "- $d$ = dimension des données\n",
    "\n",
    "## 2. Algorithme EM (Expectation-Maximization)\n",
    "L’algorithme EM est une méthode itérative pour estimer les paramètres $$\\theta$$ d’un modèle de mélange lorsque certaines variables sont latentes (non observées).\n",
    "\n",
    "### 2.1 Variables Latentes et Données Complètes\n",
    "Données observées : \n",
    "$$ X = \\{x_1, \\dots, x_N\\} $$\n",
    "\n",
    "Données latentes : \n",
    "\n",
    "$Z = \\{z_1, \\dots, z_N\\}$, où $z_{nk} = 1$ si $x_n$ provient de la composante $k$, sinon $0$.\n",
    "\n",
    "La vraisemblance complète s'écrit :\n",
    "\n",
    "$$\n",
    "p(X, Z \\mid \\theta) = \\prod_{n=1}^{N} \\prod_{k=1}^{K} \\left[\\pi_k \\, p(x_n \\mid \\theta_k)\\right]^{z_{nk}}\n",
    "$$\n",
    "\n",
    "### 2.2 Étapes de l’Algorithme EM\n",
    "#### (E-Step) : Calcul des Responsabilités\n",
    "On calcule l’espérance des variables latentes $$Z$$ étant donné les paramètres courants $$\\theta^{(t)}$$ :\n",
    "\n",
    "$$\n",
    "\\gamma_{nk} = \\mathbb{E}[z_{nk} \\mid x_n, \\theta^{(t)}] = \\frac{\\pi_k^{(t)} p(x_n \\mid \\theta_k^{(t)})}{\\sum_{j=1}^{K} \\pi_j^{(t)} p(x_n \\mid \\theta_j^{(t)})}\n",
    "$$\n",
    "\n",
    "#### (M-Step) : Maximisation de la Vraisemblance\n",
    "On met à jour les paramètres en maximisant l’espérance de la log-vraisemblance complète :\n",
    "\n",
    "$$\n",
    "\\theta^{(t+1)} = \\arg \\max_{\\theta} \\mathbb{E}[ \\log p(X, Z \\mid \\theta) \\mid X, \\theta^{(t)}]\n",
    "$$\n",
    "\n",
    "Pour un GMM, les mises à jour sont explicites :\n",
    "\n",
    "- Poids des composantes :\n",
    "\n",
    "$$\n",
    "\\pi_k^{(t+1)} = \\frac{1}{N} \\sum_{n=1}^{N} \\gamma_{nk}\n",
    "$$\n",
    "\n",
    "- Moyennes :\n",
    "\n",
    "$$\n",
    "\\mu_k^{(t+1)} = \\frac{\\sum_{n=1}^{N} \\gamma_{nk} x_n}{\\sum_{n=1}^{N} \\gamma_{nk}}\n",
    "$$\n",
    "\n",
    "- Matrices de covariance :\n",
    "\n",
    "$$\n",
    "\\Sigma_k^{(t+1)} = \\frac{\\sum_{n=1}^{N} \\gamma_{nk} (x_n - \\mu_k^{(t+1)})(x_n - \\mu_k^{(t+1)})^T}{\\sum_{n=1}^{N} \\gamma_{nk}}\n",
    "$$\n",
    "\n",
    "## 3. Application : Clustering avec les CMM (Conditional Mixture Models)\n",
    "Les CMM sont une variante des modèles de mélange où chaque composante est centrée sur un point de données (exemplaire).\n",
    "\n",
    "### 3.1 Formulation du Problème\n",
    "Soit un dataset $$ X = \\{x_1, \\dots, x_N\\} $$, la distribution du mélange est :\n",
    "\n",
    "$$\n",
    "Q(x) = \\sum_{j=1}^{N} q_j f_j(x)\n",
    "$$\n",
    "\n",
    "où :\n",
    "\n",
    "- $f_j(x) = C_\\phi(x) \\exp\\left(-\\beta \\, d_\\phi(x, x_j)\\right)$ (distribution exponentielle)\n",
    "- $d_\\phi$ = divergence de Bregman (ex : distance euclidienne $\\|x - x_j\\|_2$)\n",
    "- $q_j$ = probabilité a priori du $j$-ième exemplaire\n",
    "\n",
    "### 3.2 Maximisation de la Log-Vraisemblance\n",
    "La log-vraisemblance s’écrit :\n",
    "\n",
    "$$\n",
    "L(X; \\{q_j\\}) = \\frac{1}{N} \\sum_{i=1}^{N} \\log \\left( \\sum_{j=1}^{N} q_j \\exp\\left( -\\beta d_\\phi(x_i, x_j) \\right) \\right) + \\text{const}\n",
    "$$\n",
    "\n",
    "### 3.3 Minimisation de la Divergence KL\n",
    "Le problème est équivalent à minimiser :\n",
    "\n",
    "$$\n",
    "D(P \\parallel Q) = -L + \\text{const}\n",
    "$$\n",
    "\n",
    "où $$ P $$ est la distribution empirique.\n",
    "\n",
    "### 3.4 Mise à Jour des Paramètres\n",
    "La règle de mise à jour EM pour $$ q_j $$ est :\n",
    "\n",
    "$$\n",
    "q_j^{(t+1)} = q_j^{(t)} \\frac{\\sum_{i=1}^{N} P(x_i) f_j(x_i)}{\\sum_{j'} q_{j'}^{(t)} f_{j'}(x_i)}\n",
    "$$\n",
    "\n",
    "## 4. Étude de Cas : Segmentation d’Images par GMM\n",
    "### 4.1 Problème\n",
    "Segmenter une image en 3 régions (fond, objet, bordure) en utilisant un GMM sur les pixels.\n",
    "\n",
    "### 4.2 Modélisation\n",
    "Chaque pixel $$ x_n $$ est représenté par sa couleur (RGB).\n",
    "\n",
    "On utilise un GMM à 3 composantes.\n",
    "\n",
    "### 4.3 Algorithme EM Appliqué\n",
    "**Initialisation** :\n",
    "\n",
    "- $$ \\pi_k = \\frac{1}{3} $$  \n",
    "- $ \\mu_k $ = centres issus du K-means  \n",
    "- $ \\Sigma_k $ = matrices identité\n",
    "\n",
    "**E-Step** :  \n",
    "Calculer $$ \\gamma_{nk} $$ pour chaque pixel.\n",
    "\n",
    "**M-Step** :  \n",
    "Mettre à jour $$ \\pi_k, \\mu_k, \\Sigma_k $$.\n",
    "\n",
    "**Convergence** :  \n",
    "Après ~20 itérations, les pixels sont classés en 3 clusters.\n",
    "\n",
    "### 4.4 Résultats\n",
    "- Cluster 1 = Fond (couleur uniforme)\n",
    "- Cluster 2 = Objet principal\n",
    "- Cluster 3 = Contours\n",
    "\n",
    "## 5. Conclusion et Perspectives\n",
    "Les modèles de mélange offrent une approche flexible pour modéliser des données complexes.\n",
    "\n",
    "L’algorithme EM permet une estimation efficace des paramètres même avec des données manquantes.\n",
    "\n",
    "Les CMM étendent cette approche en utilisant des exemplaires, utile pour le clustering.\n",
    "\n",
    "**Applications avancées** :\n",
    "\n",
    "- Reconnaissance vocale (HMM)\n",
    "- Imagerie médicale (segmentation)\n",
    "- Recommandation (collaborative filtering)\n",
    "\n",
    "## Références :\n",
    "\n",
    "- Bishop, *Pattern Recognition and Machine Learning* (2006)\n",
    "- Dempster et al., *Maximum Likelihood from Incomplete Data via the EM Algorithm* (1977)\n",
    "- [Paper original sur les CMM]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
