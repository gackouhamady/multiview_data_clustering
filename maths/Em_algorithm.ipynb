{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Principe Mathématique de l'Algorithme EM**\n",
    "\n",
    "L'algorithme EM (Expectation-Maximization) est une méthode itérative pour estimer les paramètres $$\\theta$$ d'un modèle probabiliste lorsque certaines variables sont latentes (non observées).  \n",
    "\n",
    "### **Formalisation**\n",
    "1. **Modèle de mélange** :  \n",
    "   Les données $$X = \\{x_1, ..., x_N\\}$$ sont supposées générées par un mélange de $$K$$ distributions :  \n",
    "   $$\n",
    "   p(x|\\theta) = \\sum_{k=1}^K \\pi_k \\, p(x|\\theta_k), \\quad \\text{avec } \\sum_{k=1}^K \\pi_k = 1.\n",
    "   $$\n",
    "\n",
    "2. **Variables latentes** :  \n",
    "   On introduit $$Z = \\{z_{nk}\\}$$, où $$z_{nk} = 1$$ si $$x_n$$ provient de la composante $$k$$, sinon $$0$$.\n",
    "\n",
    "3. **Étapes de l'EM** :  \n",
    "   - **E-Step** : Calcul des responsabilités $$\\gamma_{nk} = \\mathbb{E}[z_{nk}|X, \\theta^{(t)}]$$.  \n",
    "     $$\n",
    "     \\gamma_{nk} = \\frac{\\pi_k \\, p(x_n|\\theta_k^{(t)})}{\\sum_{j=1}^K \\pi_j \\, p(x_n|\\theta_j^{(t)})}.\n",
    "     $$\n",
    "   - **M-Step** : Maximisation de l'espérance de la log-vraisemblance complète :  \n",
    "     $$\n",
    "     \\theta^{(t+1)} = \\arg\\max_\\theta \\sum_{n=1}^N \\sum_{k=1}^K \\gamma_{nk} \\ln \\left( \\pi_k \\, p(x_n|\\theta_k) \\right).\n",
    "     $$\n",
    "\n",
    "### **Convergence**\n",
    "L'algorithme garantit une amélioration monotone de la vraisemblance jusqu'à convergence vers un maximum local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple Complet d'Application de l'Algorithme EM sur un Mélange de Gaussiennes (GMM)\n",
    "\n",
    "## Énoncé du Problème\n",
    "\n",
    "Supposons que nous avons 4 points de données unidimensionnels :\n",
    "\n",
    "$$X = \\{x_1 = 1, x_2 = 2, x_3 = 5, x_4 = 6\\}$$\n",
    "\n",
    "Nous voulons modéliser ces données comme un mélange de deux Gaussiennes ($$K=2$$) avec :\n",
    "\n",
    "- **Moyennes initiales** : $$\\mu_1=2, \\mu_2=5$$\n",
    "- **Variances initiales** : $$\\sigma_1^2=1, \\sigma_2^2=1$$\n",
    "- **Proportions initiales** : $$\\pi_1=0.5, \\pi_2=0.5$$\n",
    "\n",
    "**But** : Estimer les paramètres finaux ($$\\mu_k, \\sigma_k^2, \\pi_k$$) en utilisant l'algorithme EM.\n",
    "\n",
    "---\n",
    "\n",
    "## Étape 1 : Initialisation\n",
    "\n",
    "$$\\theta^{(0)} = \\{ \\pi_1=0.5, \\pi_2=0.5, \\mu_1=2, \\mu_2=5, \\sigma_1^2=1, \\sigma_2^2=1 \\}$$\n",
    "\n",
    "---\n",
    "\n",
    "## Étape 2 : Itération 1 (E-Step → M-Step)\n",
    "\n",
    "### E-Step : Calcul des Responsabilités ($$\\gamma_{nk}$$)\n",
    "\n",
    "Pour chaque point $$x_n$$, calculer :\n",
    "\n",
    "$$\\gamma_{nk} = \\frac{\\pi_k \\cdot N(x_n | \\mu_k, \\sigma_k^2)}{\\sum_{j=1}^{2} \\pi_j \\cdot N(x_n | \\mu_j, \\sigma_j^2)}$$\n",
    "\n",
    "où $$N(x | \\mu, \\sigma^2)$$ est la densité Gaussienne.\n",
    "\n",
    "**Calcul pour $$x_1 = 1$$** :\n",
    "\n",
    "$$N(1 | \\mu_1=2, \\sigma_1^2=1) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{(1-2)^2}{2}} \\approx 0.2419$$\n",
    "\n",
    "$$N(1 | \\mu_2=5, \\sigma_2^2=1) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{(1-5)^2}{2}} \\approx 0.0004$$\n",
    "\n",
    "$$\\gamma_{11} = \\frac{0.5 \\times 0.2419}{0.5 \\times 0.2419 + 0.5 \\times 0.0004} \\approx 0.9983$$\n",
    "\n",
    "$$\\gamma_{12} = 1 - \\gamma_{11} \\approx 0.0017$$\n",
    "\n",
    "De même pour les autres points :\n",
    "\n",
    "| $$x_n$$ | $$\\gamma_{n1}$$ | $$\\gamma_{n2}$$ |\n",
    "|------|--------------|--------------|\n",
    "| 1    | 0.9983       | 0.0017       |\n",
    "| 2    | 0.9538       | 0.0462       |\n",
    "| 5    | 0.0462       | 0.9538       |\n",
    "| 6    | 0.0017       | 0.9983       |\n",
    "\n",
    "### M-Step : Mise à Jour des Paramètres\n",
    "\n",
    "Nouvelles proportions ($$\\pi_k$$) :\n",
    "\n",
    "$$\\pi_k^{new} = \\frac{1}{N} \\sum_{n=1}^{4} \\gamma_{nk}$$\n",
    "\n",
    "$$\\pi_1^{new} = \\frac{0.9983 + 0.9538 + 0.0462 + 0.0017}{4} \\approx 0.5$$\n",
    "\n",
    "$$\\pi_2^{new} = 1 - \\pi_1^{new} = 0.5$$\n",
    "\n",
    "Nouvelles moyennes ($$\\mu_k$$) :\n",
    "\n",
    "$$\\mu_k^{new} = \\frac{\\sum_{n=1}^{4} \\gamma_{nk} x_n}{\\sum_{n=1}^{4} \\gamma_{nk}}$$\n",
    "\n",
    "$$\\mu_1^{new} = \\frac{0.9983 \\times 1 + 0.9538 \\times 2 + 0.0462 \\times 5 + 0.0017 \\times 6}{2} \\approx 1.51$$\n",
    "\n",
    "$$\\mu_2^{new} = \\frac{0.0017 \\times 1 + 0.0462 \\times 2 + 0.9538 \\times 5 + 0.9983 \\times 6}{2} \\approx 5.49$$\n",
    "\n",
    "Nouvelles variances ($$\\sigma_k^2$$) :\n",
    "\n",
    "$$\\sigma_k^2 = \\frac{\\sum_{n=1}^{4} \\gamma_{nk} (x_n - \\mu_k^{new})^2}{\\sum_{n=1}^{4} \\gamma_{nk}}$$\n",
    "\n",
    "$$\\sigma_1^2 \\approx 0.25, \\quad \\sigma_2^2 \\approx 0.25$$\n",
    "\n",
    "Nouveaux paramètres après Itération 1 :\n",
    "\n",
    "$$\\theta^{(1)} = \\{ \\pi_1=0.5, \\pi_2=0.5, \\mu_1 \\approx 1.51, \\mu_2 \\approx 5.49, \\sigma_1^2 \\approx 0.25, \\sigma_2^2 \\approx 0.25 \\}$$\n",
    "\n",
    "---\n",
    "\n",
    "## Itération 2 (Convergence Rapide)\n",
    "\n",
    "En répétant l'E-Step et M-Step, les paramètres convergent vers :\n",
    "\n",
    "$$\\theta^{final} = \\{ \\pi_1=0.5, \\pi_2=0.5, \\mu_1 \\approx 1.5, \\mu_2 \\approx 5.5, \\sigma_1^2 \\approx 0.25, \\sigma_2^2 \\approx 0.25 \\}$$\n",
    "\n",
    "### Interprétation :\n",
    "\n",
    "- **Cluster 1** : Points autour de $$1.5$$ ($$\\{1,2\\}$$).\n",
    "- **Cluster 2** : Points autour de $$5.5$$ ($$\\{5,6\\}$$).\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "L'algorithme EM a correctement identifié les deux sous-populations dans les données.\n",
    "\n",
    "- **E-Step** : Calcule les probabilités d'appartenance.\n",
    "- **M-Step** : Met à jour les paramètres pour maximiser la vraisemblance.\n",
    "\n",
    "**Application Réelle** : Segmentation de clients, détection d'anomalies, etc."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
