{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Author**  : [Victor Lavrenko](https://www.youtube.com/watch?v=REypj2sy_5U&list=PLBv09BD7ez_4e9LtmK626Evn1ion6ynrt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Mixture models**\n",
    "\n",
    "# Clustering Methods Overview / Aperçu des Méthodes de Clustering\n",
    "\n",
    "## English Version\n",
    "\n",
    "### Types of Clustering Methods\n",
    "- **Hard Clustering**:\n",
    "  - Clusters are mutually exclusive\n",
    "  - Each element belongs to exactly one cluster\n",
    "  - Example: K-means algorithm\n",
    "\n",
    "- **Soft Clustering**:\n",
    "  - Clusters may overlap\n",
    "  - Elements have probabilistic membership across clusters\n",
    "  - Measures strength of association between instances and clusters\n",
    "\n",
    "### Mixture Models\n",
    "- Probabilistic framework for soft clustering\n",
    "- Each cluster represented by:\n",
    "  - Gaussian distribution (for continuous data)\n",
    "  - Multinomial distribution (for discrete data)\n",
    "- Key unknown parameters:\n",
    "  - For Gaussians: $$ \\mu_k \\text{ (mean)}, \\Sigma_k \\text{ (covariance)} $$\n",
    "  - For Multinomials: $$ \\theta_k \\text{ (probability vector)} $$\n",
    "\n",
    "### Expectation-Maximization Algorithm\n",
    "- Iterative parameter estimation:\n",
    "  1. **E-step**: Compute posterior probabilities\n",
    "     $$ P(z_i = k|x_i) = \\frac{P(x_i|z_i=k)P(z_i=k)}{\\sum_{j=1}^K P(x_i|z_i=j)P(z_i=j)} $$\n",
    "  2. **M-step**: Update parameters\n",
    "     $$ \\mu_k^{new} = \\frac{\\sum_i P(z_i=k|x_i)x_i}{\\sum_i P(z_i=k|x_i)} $$\n",
    "- Automatically discovers all parameters for K components\n",
    "\n",
    "## Version Française\n",
    "\n",
    "### Types de Méthodes de Clustering\n",
    "- **Clustering Dur** :\n",
    "  - Clusters mutuellement exclusifs\n",
    "  - Chaque élément appartient à exactement un cluster\n",
    "  - Exemple : Algorithme K-means\n",
    "\n",
    "- **Clustering Mou** :\n",
    "  - Clusters peuvent se chevaucher\n",
    "  - Appartenance probabiliste aux différents clusters\n",
    "  - Mesure la force d'association entre instances et clusters\n",
    "\n",
    "### Modèles de Mélange\n",
    "- Approche probabiliste pour le clustering mou\n",
    "- Chaque cluster représenté par :\n",
    "  - Loi normale (données continues)\n",
    "  - Loi multinomiale (données discrètes)\n",
    "- Paramètres inconnus :\n",
    "  - Pour Gaussiennes : $$ \\mu_k \\text{ (moyenne)}, \\Sigma_k \\text{ (matrice de covariance)} $$\n",
    "  - Pour Multinomiales : $$ \\theta_k \\text{ (vecteur de probabilités)} $$\n",
    "\n",
    "### Algorithme EM (Expectation-Maximisation)\n",
    "- Estimation itérative des paramètres :\n",
    "  1. **Étape E** : Calcule les probabilités a posteriori\n",
    "     $$ P(z_i = k|x_i) = \\frac{P(x_i|z_i=k)P(z_i=k)}{\\sum_{j=1}^K P(x_i|z_i=j)P(z_i=j)} $$\n",
    "  2. **Étape M** : Met à jour les paramètres\n",
    "     $$ \\mu_k^{new} = \\frac{\\sum_i P(z_i=k|x_i)x_i}{\\sum_i P(z_i=k|x_i)} $$\n",
    "- Découverte automatique des paramètres pour K composantes\n",
    "\n",
    "**Note** : Toutes les expressions mathématiques sont encadrées par `$$ $$` comme demandé, avec une correspondance exacte entre les versions anglaise et française.\n",
    "\n",
    "\n",
    "## Mixture models in 1-d\n",
    "\n",
    "- Observations $$ ( x_1, \\ldots, x_n )$$\n",
    "  - K=2 Gaussians with unknown $$(\\mu) $$,  $$(\\sigma^2)$$\n",
    "  - estimation trivial if we know the source of each observation\n",
    "\n",
    "$$\\mu_b = \\frac{x_1 + x_2 + \\ldots + x_{n_b}}{n_b}$$\n",
    "\n",
    "$$\\sigma_b^2 = \\frac{(x_1 - \\mu_1)^2 + \\ldots + (x_n - \\mu_n)^2}{n_b}$$\n",
    "\n",
    "---\n",
    "\n",
    "- What if we don’t know the source?\n",
    "\n",
    "- If we knew parameters of the Gaussians $$ (\\mu) $$, $$ (\\sigma^2)$$\n",
    "  - can guess whether point is more likely to be a or b\n",
    "\n",
    "$$P(b | x_i) = \\frac{P(x_i | b) P(b)}{P(x_i | b) P(b) + P(x_i | a) P(a)}$$\n",
    "\n",
    "$$T(x_i | 0) = \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}}\\exp\\left(-\\frac{(x - \\mu_i)^2}{2\\sigma_i^2}\\right)$$\n",
    "\n",
    "\n",
    "## Expectation Maximization (EM)\n",
    "\n",
    "- Chicken and egg problem  \n",
    "  - need $$((\\mu_a, \\sigma_a^2)) $$ and  $$ ((\\mu_b, \\sigma_b^2)) $$ to guess source of points  \n",
    "  - need to know source to estimate $$((\\mu_a, \\sigma_a^2))$$ and $$((\\mu_b, \\sigma_b^2))$$\n",
    "\n",
    "- EM algorithm  \n",
    "  - start with two randomly placed Gaussians $$((\\mu_a, \\sigma_a^2))$$, $$((\\mu_b, \\sigma_b^2))$$\n",
    "\n",
    "**E-step:**  \n",
    "  - for each point: $ (\\P(b|x_i) =)$ does it look like it came from b?\n",
    "\n",
    "**M-step:**  \n",
    "  - adjust $$((\\mu_a, \\sigma_a^2)) $$ and $$ ((\\mu_b, \\sigma_b^2)) $$ to fit points assigned to them  \n",
    "  - iterate until convergence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Expectation Maximization: How It Works\n",
    "\n",
    "### EM Algorithm in 1D Gaussian Mixture Models\n",
    "\n",
    "#### Mathematical Framework\n",
    "\n",
    "The EM algorithm alternates between two steps:\n",
    "\n",
    "1. **E-step (Expectation)**:\n",
    "   - Compute posterior probabilities for each point belonging to each cluster\n",
    "   \n",
    "   $$P(x_i | b) = \\frac{1}{\\sqrt{2\\pi\\sigma_b^2}} \\exp\\left( -\\frac{(x_i - \\mu_b)^2}{2\\sigma_b^2} \\right)$$\n",
    "   \n",
    "   $$b_i = P(b | x_i) = \\frac{P(x_i | b)P(b)}{P(x_i | b)P(b) + P(x_i | a)P(a)}$$\n",
    "   \n",
    "   $$a_i = P(a | x_i) = 1 - b_i$$\n",
    "\n",
    "2. **M-step (Maximization)**:\n",
    "   - Update cluster parameters using weighted averages\n",
    "   \n",
    "   $$\\mu_b = \\frac{\\sum_{i=1}^n b_i x_i}{\\sum_{i=1}^n b_i}$$\n",
    "   \n",
    "   $$\\sigma_b^2 = \\frac{\\sum_{i=1}^n b_i(x_i - \\mu_b)^2}{\\sum_{i=1}^n b_i}$$\n",
    "   \n",
    "   (Analogous formulas for cluster a)\n",
    "\n",
    "\n",
    "#### Algorithm Commentary\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Start with random cluster parameters (μ,σ²)\n",
    "   - Typically set equal priors: P(a) = P(b) = 0.5\n",
    "\n",
    "2. **Iteration Process**:\n",
    "   - E-step \"soft assigns\" points using current parameters\n",
    "   - M-step re-estimates parameters using these soft assignments\n",
    "   - Each iteration increases the log-likelihood\n",
    "\n",
    "3. **Convergence**:\n",
    "   - Stops when parameter changes < ε or likelihood plateaus\n",
    "   - Final output includes:\n",
    "     - Cluster parameters (μ,σ²)\n",
    "     - Posterior probabilities b_i for each point\n",
    "     - Estimated priors: \n",
    "       $$P(b) = \\frac{\\sum b_i}{n}$$\n",
    "       $$P(a) = 1 - P(b)$$\n",
    "\n",
    "#### Key Properties\n",
    "- Guaranteed to converge to local optimum\n",
    "- Sensitive to initialization (may need multiple restarts)\n",
    "- Naturally handles soft assignments through probabilities\n",
    "\n",
    "\n",
    "\n",
    "## Gaussian Mixture Models: Multivariate Case (d>1)\n",
    "\n",
    "### English Version\n",
    "\n",
    "#### Model Description\n",
    "- Data with **d attributes** from **k sources**\n",
    "- Each source **c** is a multivariate Gaussian distribution\n",
    "- Parameters estimated iteratively:\n",
    "\n",
    "1. **Prior Probability** (Cluster weight):\n",
    "   $$ P(c) = \\frac{1}{n} \\sum_{i=1}^{n} P(c | \\vec{x}_i) $$\n",
    "\n",
    "2. **Mean Vector** (Cluster center):\n",
    "   $$ \\mu_{c,j} = \\sum_{i=1}^{n} \\left( \\frac{P(c|\\vec{x}_i)}{nP(c)} \\right) x_{i,j} $$\n",
    "\n",
    "3. **Covariance Matrix** (Attribute relationships):\n",
    "   $$ (\\Sigma_c)_{j,k} = \\sum_{i=1}^{n} \\left( \\frac{P(c|\\vec{x}_i)}{nP(c)} \\right) (x_{i,j} - \\mu_{c,j})(x_{i,k} - \\mu_{c,k}) $$\n",
    "\n",
    "#### Key Equations\n",
    "- **Posterior Probability** (Cluster membership):\n",
    "  $$ P(c | \\vec{x}_i) = \\frac{P(\\vec{x}_i | c)P(c)}{\\sum_{c'=1}^{k} P(\\vec{x}_i | c') P(c')} $$\n",
    "\n",
    "- **Gaussian Density**:\n",
    "  $$ P(\\vec{x}_i | c) = \\frac{1}{\\sqrt{(2\\pi)^d |\\Sigma_c|}} \\exp\\left(-\\frac{1}{2} (\\vec{x}_i - \\vec{\\mu}_c)^T \\Sigma_c^{-1} (\\vec{x}_i - \\vec{\\mu}_c)\\right) $$\n",
    "\n",
    "---\n",
    "\n",
    "### Version Française\n",
    "\n",
    "#### Description du Modèle\n",
    "- Données à **d attributs** provenant de **k sources**\n",
    "- Chaque source **c** suit une loi normale multivariée\n",
    "- Paramètres estimés itérativement :\n",
    "\n",
    "1. **Probabilité a priori** (Poids du cluster) :\n",
    "   $$ P(c) = \\frac{1}{n} \\sum_{i=1}^{n} P(c | \\vec{x}_i) $$\n",
    "\n",
    "2. **Vecteur moyen** (Centre du cluster) :\n",
    "   $$ \\mu_{c,j} = \\sum_{i=1}^{n} \\left( \\frac{P(c|\\vec{x}_i)}{nP(c)} \\right) x_{i,j} $$\n",
    "\n",
    "3. **Matrice de covariance** (Relations entre attributs) :\n",
    "   $$ (\\Sigma_c)_{j,k} = \\sum_{i=1}^{n} \\left( \\frac{P(c|\\vec{x}_i)}{nP(c)} \\right) (x_{i,j} - \\mu_{c,j})(x_{i,k} - \\mu_{c,k}) $$\n",
    "\n",
    "#### Équations Clés\n",
    "- **Probabilité a posteriori** (Appartenance au cluster) :\n",
    "  $$ P(c | \\vec{x}_i) = \\frac{P(\\vec{x}_i | c)P(c)}{\\sum_{c'=1}^{k} P(\\vec{x}_i | c') P(c')} $$\n",
    "\n",
    "- **Densité Gaussienne** :\n",
    "  $$ P(\\vec{x}_i | c) = \\frac{1}{\\sqrt{(2\\pi)^d |\\Sigma_c|}} \\exp\\left(-\\frac{1}{2} (\\vec{x}_i - \\vec{\\mu}_c)^T \\Sigma_c^{-1} (\\vec{x}_i - \\vec{\\mu}_c)\\right) $$\n",
    "\n",
    "#### Notes Techniques\n",
    "- Toutes les formules sont présentées en notation vectorielle/matricielle\n",
    "- La normalisation inclut le déterminant $|\\Sigma_c|$ pour d>1\n",
    "- L'exposant $T$ désigne la transposition vectorielle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## How to Choose the Number of Clusters \\( K \\)? / Comment choisir le nombre de clusters \\( K \\) ?\n",
    "\n",
    "### English Version\n",
    "\n",
    "#### Probabilistic Approach\n",
    "The model's log-likelihood measures how well it fits the data:\n",
    "$$ L = \\log P(x_1, ..., x_n) = \\sum_{i=1}^n \\log \\sum_{k=1}^{K} P(x_i | k) P(k) $$\n",
    "\n",
    "#### Challenges in Selecting \\( K \\)\n",
    "- **Overfitting Risk**: Choosing \\( K = n \\) (one cluster per point) maximizes likelihood but generalizes poorly:\n",
    "  $$ \\text{When } K = n: \\quad P(x_i|k) = \\begin{cases} \n",
    "  1 & \\text{if } i = k \\\\\n",
    "  0 & \\text{otherwise}\n",
    "  \\end{cases} $$\n",
    "\n",
    "#### Practical Methods\n",
    "1. **Train-Validation Split**:\n",
    "   - Fit on training set \\( T \\), evaluate on validation set \\( V \\)\n",
    "   $$ L_V(K) = \\sum_{x_i \\in V} \\log \\sum_{k=1}^K P_T(x_i|k)P_T(k) $$\n",
    "\n",
    "2. **Information Criteria**:\n",
    "   - **Bayesian Information Criterion (BIC)**:\n",
    "     $$ \\text{BIC}(K) = L(K) - \\frac{\\gamma}{2} p_K \\log n $$\n",
    "   - **Akaike Information Criterion (AIC)**:\n",
    "     $$ \\text{AIC}(K) = 2 p_K - L(K) $$\n",
    "\n",
    "  Where:\n",
    "- $p_K = K\\left(d + \\frac{d(d+1)}{2}\\right)$ parameters for $d$-dim. Gaussians\n",
    "- $\\gamma$: penalty coefficient (typically 1)\n",
    "\n",
    "---\n",
    "\n",
    "### Version Française\n",
    "\n",
    "#### Approche Probabiliste\n",
    "La log-vraisemblance mesure l'adéquation du modèle aux données :\n",
    "$$ L = \\log P(x_1, ..., x_n) = \\sum_{i=1}^n \\log \\sum_{k=1}^{K} P(x_i | k) P(k) $$\n",
    "\n",
    "#### Problématiques du Choix de \\( K \\)\n",
    "- **Risque de surapprentissage** : \\( K = n \\) (un cluster par point) maximise \\( L \\) mais généralise mal :\n",
    "  $$ \\text{Quand } K = n : \\quad P(x_i|k) = \\begin{cases} \n",
    "  1 & \\text{si } i = k \\\\\n",
    "  0 & \\text{sinon}\n",
    "  \\end{cases} $$\n",
    "\n",
    "#### Méthodes Pratiques\n",
    "1. **Validation Croisée** :\n",
    "   - Apprentissage sur \\( T \\), évaluation sur \\( V \\)\n",
    "   $$ L_V(K) = \\sum_{x_i \\in V} \\log \\sum_{k=1}^K P_T(x_i|k)P_T(k) $$\n",
    "\n",
    "2. **Critères d'Information** :\n",
    "   - **Critère BIC** (Bayésien) :\n",
    "     $$ \\text{BIC}(K) = L(K) - \\frac{\\gamma}{2} p_K \\log n $$\n",
    "   - **Critère AIC** (Akaike) :\n",
    "     $$ \\text{AIC}(K) = 2 p_K - L(K) $$\n",
    "\n",
    "   Où :\n",
    "  - $p_K = K \\left(d + \\frac{d(d+1)}{2}\\right)$ paramètres pour des Gaussiennes $d$-dimensionnelles\n",
    "- $\\gamma$ : coefficient de pénalité (généralement 1)\n",
    "\n",
    "#### Glossaire / Glossary\n",
    "| Term          | Description                          |\n",
    "|---------------|--------------------------------------|\n",
    "| **Likelihood** (Vraisemblance) | How well the model fits the data |\n",
    "| **Overfitting** (Surapprentissage) | Model too complex for the data |\n",
    "| **Occam's razor** (Rasoir d'Occam) | Prefer simpler models when possible |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Summary / Résumé\n",
    "\n",
    "## English Version\n",
    "\n",
    "### Key Features of EM Algorithm\n",
    "- **Generalization**:\n",
    "  - Works in **1D** → Extends to **d-dimensional Gaussians** (non-spherical)\n",
    "  - Handles **discrete data** (e.g., text via pLSI with multinomial distributions)\n",
    "\n",
    "- **Objective Function**:\n",
    "  $$ P(x_1...x_n) = \\prod_{i=1}^n \\sum_{k=1}^K P(x_i | k)P(k) $$\n",
    "  Maximizes the **likelihood** of observed data.\n",
    "\n",
    "### Comparison with K-means\n",
    "| Feature               | EM Algorithm                          | K-means                     |\n",
    "|-----------------------|---------------------------------------|-----------------------------|\n",
    "| **Clustering Type**   | Soft (probabilistic assignments)      | Hard (strict assignments)   |\n",
    "| **Distance Metric**   | Adaptive (covariance matrices)        | Fixed (usually Euclidean)   |\n",
    "| **Convergence**       | Local optimum (likelihood-based)      | Local optimum (SSE-based)   |\n",
    "| **K Selection**       | Cannot auto-discover (use BIC/AIC)    | Requires manual choice      |\n",
    "\n",
    "### Limitations\n",
    "- **Initialization Sensitivity**:  \n",
    "  Converges to local maxima (random restarts recommended)\n",
    "- **Convergence Criterion**:  \n",
    "  Stops when $$( \\Delta P(x_1...x_n) < \\epsilon )$$\n",
    "- **Model Complexity**:  \n",
    "  Likelihood always increases with \\( K \\) → Requires external validation.\n",
    "\n",
    "---\n",
    "\n",
    "## Version Française\n",
    "\n",
    "### Caractéristiques Clés\n",
    "- **Généralisation** :\n",
    "  - Fonctionne en **1D** → Étendue aux **Gaussiennes multidimensionnelles**\n",
    "  - Gère les **données discrètes** (ex: textes via modèles multinomiaux pLSI)\n",
    "\n",
    "- **Fonction Objectif** :\n",
    "  $$ P(x_1...x_n) = \\prod_{i=1}^n \\sum_{k=1}^K P(x_i | k)P(k) $$\n",
    "  Maximise la **vraisemblance** des données.\n",
    "\n",
    "### Comparaison avec K-means\n",
    "| Aspect                | Algorithme EM                        | K-means                     |\n",
    "|-----------------------|---------------------------------------|-----------------------------|\n",
    "| **Type de Clustering**| Mou (assignations probabilistes)      | Dur (assignations strictes) |\n",
    "| **Métrique**          | Adaptative (matrices de covariance)   | Fixe (souvent Euclidienne)  |\n",
    "| **Convergence**       | Optimum local (vraisemblance)         | Optimum local (SSE)         |\n",
    "| **Choix de K**        | Non auto-déterminé (utiliser BIC/AIC) | Choix manuel requis         |\n",
    "\n",
    "### Limitations\n",
    "- **Sensibilité à l'Initialisation** :  \n",
    "  Converge vers des maxima locaux (relances aléatoires conseillées)\n",
    "- **Critère d'Arrêt** :  \n",
    "  Stoppe quand $$( \\Delta P(x_1...x_n) < \\epsilon )$$\n",
    "- **Complexité** :  \n",
    "  La vraisemblance croît toujours avec \\( K \\) → Validation externe nécessaire.\n",
    "\n",
    "---\n",
    "\n",
    "**Visualization Suggestion**:\n",
    "1. Side-by-side comparison of EM (soft) vs K-means (hard) clustering\n",
    "2. Plot showing likelihood vs K with BIC/AIC thresholds\n",
    "3. Animation of covariance adaptation during EM iterations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
